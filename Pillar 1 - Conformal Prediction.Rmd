---
title: "Pillar 1 - Conformal Prediction"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
editor_options: 
  chunk_output_type: console
---

# Set up

```{r setup, include = FALSE}
set.seed(1)

knitr::opts_chunk$set(fig.width = 3, fig.height = 2,
                      echo = FALSE, warning = FALSE)

library(tidyverse)
library(glmnet)

load("Output/CV.RDATA")
load("Output/data_student.RDATA")
load("Output/student_profile.RDATA")
```

```{r setup_graph}
theme_set(theme_light())

output_folder <- "Conference/COPA2019/paper/figures"
plot_width  <- 6
plot_height <- plot_width / 1.6 
```


# Course selection
I select courses with large and small n, and large and small CV error. I do not select courses whose lasso model contains less than 3 non-zero coefficients (df).

* COR1002: large n
* COR1004: large n and smaller CV error (mean absolute error) than COR1002

* SCI3003: small n and large CV error
* SCI2040: small n and small CV erro

* SSC3044: small CV error
* SSC3038: small CV error and n twice as large as SSC3044

* SCI2018: large CV error
* SCI2010: large CV error and n twice as large as SCI2018


```{r course_selection}
course_selection <- c(
#  "SCI3003", "SCI2040", # remove because n too small
  "COR1002", "COR1004",  "SSC3044", "SSC3038", "SCI2018", "SCI2010")

course_table <- fit_lasso %>% 
  filter(target %in% course_selection) %>% 
  select(target, n, cv_error, df)
```

# Conformal Prediction setp by step

## Cross-validation

```{r CV helper}
create_Z <- function(course){
  
  student_profile %>%
    filter(`Course ID` == course) %>%
    select(Y = Grade, matches("GPA|Topic"))
 
}

add_fold <- function(df, n_fold = 10){
  
  n         <- nrow(df)
  fold_size <- ceiling(n / n_fold)
  folds     <- rep(1 : 10, fold_size) %>% sample %>% .[1 : n]
  
  df %>% mutate(fold = folds)
  
}
```

## Training, callibration and test sets

```{r split helper}
create_test <- function(df, fold_current) df %>% filter(fold == fold_current)

create_training <- function(df, fold_current) df %>% filter(fold != fold_current)

add_allocation <- function(df){
  
  n_training <- nrow(df)
  n_proper_training <- floor(0.67 * n_training)        # 2/3
  n_calibration     <- n_training - n_proper_training  # 1/3
  
  allocation <- rep(x     = c("proper training", "calibration"), 
                    times = c(n_proper_training, n_calibration)) %>%
  sample
  
  df %>% mutate(allocation = allocation)
  
}

create_proper_training <- function(df) df %>% filter(allocation == "proper training")

create_calibration     <- function(df) df %>% filter(allocation == "calibration")
```

## Fit lasso on training set
```{r,lasso helper}
fit_target_model <- function(df){
  
  X <- df %>% select(matches("GPA|Topic"))
  Y <- df %>% pull(Y)
  
  cv.glmnet(x = as.matrix(X), y = Y, type.measure = "mae") # lasso by default. Use mean absolute error 
  
}

predict_grade <- function(df, model){
  
  X <- df %>% select(matches("GPA|Topic"))
  
  Y_hat <- predict.cv.glmnet(model, newx = as.matrix(X), s = "lambda.min") # use lambda that minimizes mae
  
  df %>% mutate(Y_hat = as.vector(Y_hat),
                error = abs(Y_hat - Y),
                mu    = log(error)) # take ln of absolute error
  
}

fit_error_model <- function(df){
  
  X  <- df %>% select(matches("GPA|Topic"))
  mu <- df %>% pull(mu)
  
  cv.glmnet(x = as.matrix(X), y = mu, type.measure = "mae")
  
}

predict_error <- function(df, model){
  
  X <- df %>% select(matches("GPA|Topic"))
  
  mu_hat <- predict.cv.glmnet(model, newx = as.matrix(X), s = "lambda.min")
  
  df %>% mutate(mu_hat = as.vector(mu_hat))
  
}
```

## Compute standardized non-conformity scores on callibration test
```{r}

compute_ncs              <- function(df) df %>% mutate(alpha = abs(Y - Y_hat))

compute_standardized_ncs <- function(df) df %>% mutate(sigma = exp(mu_hat),
                                                       alpha = abs(Y - Y_hat) / sigma )

compute_significance     <- function(df) df %>% mutate(significance = rank(-alpha) / nrow(.) ) # !!! double check!!!
```

## Evaluate error rate on test set

```{r}
compute_width <- function(df, significance_given){
  
  df <- df %>% filter(significance <= significance_given)
  
  min(df$alpha)
  
}

compute_interval <- function(df, ncs){
  
  df %>% mutate(
    sigma       = exp(mu_hat),
    width       = ncs * sigma,
    border_low  = Y_hat - width,
    border_high = Y_hat + width
    )
}

compute_hit <- function(df){
  df %>% mutate(hit = (border_low <= Y) & (Y <= border_high))
}
```

# Loop through all courses

```{r, echo = FALSE}

# Set up

significance <- c(0.05, seq(0.1, 0.9, 0.1))
results <- list()
n_list <- 1

for(course in course_selection){
  
  # data
  Z <- create_Z(course)
  Z <- add_fold(Z)
  
  for(iteration in 1 : 10){
    
    # proper training, calibration and test sets
    Z_test            <- create_test(Z, fold_current = iteration)
    Z_training        <- create_training(Z, fold_current = iteration) %>% add_allocation
    Z_proper_training <- create_proper_training(Z_training)
    Z_calibration     <- create_calibration(Z_training)
    
    # fit target and error model to proper training set
    model_target      <- fit_target_model(Z_proper_training)
    Z_proper_training <- predict_grade(Z_proper_training, model_target)
    model_error       <- fit_error_model(Z_proper_training)
    
    # non-conformity scores on calibration
    Z_calibration <- Z_calibration %>% 
      predict_grade(model_target) %>%
      predict_error(model_error) %>%
      compute_standardized_ncs %>%
      compute_significance
    
    # error on test set
    Z_test <- Z_test %>% 
      predict_grade(model_target) %>%
      predict_error(model_error)
  
    for(signi in significance){ # loop through significance levels
      
      ncs <- compute_width(Z_calibration, signi)

      Z_test_augmented <- Z_test %>% 
        compute_interval(ncs = ncs) %>%
        compute_hit
      
      # save results
      results[[n_list]] <- Z_test_augmented %>%
        mutate(significance = signi,
               iteration    = iteration,
               course       = course) %>%
        select(course, iteration, significance, width, hit,
               Y, Y_hat, error, mu, mu_hat, sigma)
      
      n_list <- n_list + 1
      
    } # end loop significance
    
  } # end CV fold

}
```

```{r}
conformal_results <- bind_rows(results) %>%
    mutate(iteration    = as.factor(iteration),
           course       = as.factor(course))

save(conformal_results, course_table, file = "Output/conformal.RDATA")
```

Illustration of the effect of error_hat on interval's width (for significance of 99%):
```{r, fig.height=4}
Z_test %>% select(Y, Y_hat, error, error_hat, width, border_low, border_high, hit)
plot(Z_test$error, Z_test$error_hat)
```

Correlation between the actual errors and the predicted errors (error_hat):
```{r}
cor(Z_test$error, Z_test$error_hat)
```

```{r, echo = FALSE, eval = F}
ggplot(data = Z_callibration) + geom_histogram(aes(Y), bins = 15)

ggplot(data = Z_callibration) + geom_histogram(aes(Y_hat), bins = 15)

ggplot(data = Z_callibration) + geom_histogram(aes(alpha), bins = 15)
```

```{r, echo = FALSE, eval = F}
ggplot(data = Z_callibration) + geom_point(aes(Y, Y_hat))

ggplot(data = Z_callibration) + geom_point(aes(Y, alpha))

ggplot(data = Z_callibration) + geom_point(aes(Y_hat, alpha))
```
