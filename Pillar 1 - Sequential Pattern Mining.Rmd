---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---

```{r library, message = F}
library(arulesSequences)

library(tidyverse)
```

# Setup
load previous data and create function to find courses
```{r loading data}
load("Output/data_pillar_1.RDATA")
```

Function that returns title of a course given its code.
```{r function find_course}
find_course <- function(code){ 
  
  dataset <- d_transcript %>%
    filter(`Course ID`== code)
  
  title <- dataset$`Course Title`[1]
  
  return(title)
  
}

# Example
find_course("HUM1005")
```

# Data Exploration

```{r statistic summary}

# For convenience
provide_statistics <- function(data){
  data %>%
    summarise(
      Min       = min(Grade),
      Max       = max(Grade), 
      Mean      = round(mean(Grade), 2), 
      Median    = median(Grade), 
      SD        = round(sd(Grade), 2),
      Fail_rate = round(mean(Fail), 2),
      Fail_n    = sum(Fail),
      n         = n()
      )
}

# Student level
statistics_student <- d_transcript %>%
  group_by(`Student ID`) %>%
  provide_statistics()

# Course level
statistics_course <- d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  group_by(`Course ID`) %>%
  provide_statistics()

# Cluster level
statistics_cluster <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  provide_statistics()

# Concentration evel
statistics_concentration <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  gather(X, Concentration, Concentration, `Concentration (additional)`, na.rm = TRUE) %>%
  group_by(Concentration) %>%
  provide_statistics()

# Year level
statistics_year <- d_transcript %>%
  group_by(Year_numerical) %>%
  provide_statistics()

# Level level
statistics_level <- d_transcript %>%
  # TODO: filter for student who completed their studies
  inner_join(d_course, by = c("Course ID")) %>%
  filter(!is.na(Level)) %>%
  group_by(Level) %>%
  provide_statistics()

#
# output
save(statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level, file = "Output/Transcript Statistics.RDATA")
rm(provide_statistics, statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level)
```

# Association Rules and Sequence Rules
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

## Data Prep: Creating Transactions and Sequences
First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 

```{r AR data prep 1}

#
# Threshold: pass grade, high grade
pass_grade <- 5.5
high_grade <- 6.5



#
# Transactions
d_transactions <- d_course %>%
  
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  select(- Period) %>%
  inner_join(d_transcript, by = "Course ID") %>%
  
  # Identifying sequences
  rename(sequenceID = `Student ID`) %>%
  
  # Identifying time of event (sequence)
  mutate(
    Period  = substr(Period, 1, 1),
    eventID = as.numeric(paste(Year_numerical, Period, sep = ""))
    ) %>%

  # Identifying item
  mutate(
    PF = case_when( Grade <  pass_grade ~ "fail",
                    Grade >= pass_grade ~ "pass"),
    HL = case_when( Grade <  high_grade ~ "low",
                    Grade >= high_grade ~ "high"),
    
    item_PF = paste(`Course ID`, PF, sep = "_"),
    item_HL = paste(`Course ID`, HL, sep = "_")
    ) %>%
  rename(
    item = `Course ID`
  )
```


### Transactions for Apriori
```{r making transactions}

#
# For convenience
make_transaction <- function(data = d_transactions, item = item){
  
  data <- data %>%
    group_by(
      sequenceID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    ungroup
  
  transactions <- as(
    data$list_item,
    "transactions"
    )
  
  return(transactions)
  
}


#
# Making transactions
transactions    <- make_transaction(item = item   )
transactions_PF <- make_transaction(item = item_PF)
transactions_HL <- make_transaction(item = item_HL)


#
# Checking transactions
inspect(head(transactions, 10))

rm(make_transaction)
```

### Sequences for CSPADE
```{r}

#
# For convenience
make_sequence <- function(data = d_transactions, item = item){
  
  data <- data %>%
    group_by(
      sequenceID,
      eventID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    arrange(
      sequenceID,
      eventID
    ) %>%
    ungroup
  
  sequences <- as(
    data$list_item,
    "transactions"
    )
  
  sequences@itemsetInfo <- select(
    data,
    sequenceID,
    eventID
    ) %>%
    as.data.frame()
  
  return(sequences)
  
}


#
# Making sequences
sequences    <- make_sequence(item = item   )
sequences_PF <- make_sequence(item = item_PF)
sequences_HL <- make_sequence(item = item_HL)


#
# Checking sequences
inspect(head(sequences, 10))


rm(make_transaction)
```


## Mining Rules

### Apriori Algorithm
```{r}

# Transform AR from apriori into a readable data frame
clean_AR <- function(AR){
  
  AR %>%
    as("data.frame") %>%
    filter(
      count >= 1
    ) %>%
    mutate(
      rules = str_remove(rules, pattern = "_fail"),
      rules = str_remove(rules, pattern = "_low"),
      lhs = substr(rules, 2, 8),
      rhs = substr(rules, 15, 21),
      `rhs.support` = confidence / lift
      ) %>%
    mutate_at(
      c("support", "lhs.support", "confidence", "rhs.support", "lift"),
      funs(round(., 5))
      ) %>%
    select(
      lhs, rhs,
      support, count, lhs.support,
      confidence, rhs.support,
      lift
      ) %>%
    arrange(
      desc(count)
    )
  
}
```

We apply Apriori algorithm:
```{r AR apriori}

# Run the apriori algorithm with desired parameters
my_apriori <- function(data, appearance = NULL){
  
  data %>%
    apriori(
      parameter = list(
        supp = 0,    # min support
        smax = 1,    # max support
        conf = 0,    # min confidence
        minlen = 2,  # min length of 
        maxlen = 2,  # max length of rule
        ext = TRUE,
        arem = "diff",
        aval = TRUE,
        minval = 0
        ),
      appearance = appearance,
      control = list(
        verbose = FALSE
        )
      ) %>%
    clean_AR
  
}


#
# Generating asocciation rules
AR <- my_apriori(transactions)

# creating vector of fail course
course_id_fail <- d_transactions %>%
  filter(PF == "fail") %>%
  distinct(`item_PF`)

AR_PF <- my_apriori(transactions_PF,
                    appearance = list(both = course_id_fail$`item_PF`))

# creating vector of fail course
course_id_low <- d_transactions %>%
  filter(HL == "low") %>%
  distinct(`item_HL`)

AR_HL <- my_apriori(transactions_HL,
                    appearance = list(both = course_id_low$`item_HL`))


#
# Save association rules
save(AR, AR_PF, AR_HL,
     file = "Output/AS.RDATA")

# TODO: MAKE APP



rm(clean_AR, my_apriori,
   transactions, transactions_PF, transactions_HL,
   course_id_fail, course_id_low
   AR, AR_PF, AR_HL)

```


### CSPADE Algorithm

```{r}

subsequences <- sequences %>%
  cspade(
    parameter = list(
      support = 0.2, # min support of a sequence
      maxsize = 1, # max number of items of an element of a sequence
      maxlen  = 2, # max number of element of a sequence
      mingap  = 1, # min time difference between consecutive element of a sequence
      maxgap  = 1e4, # max time difference between consecutive element of a sequence
      maxwin  = 1e4  # max time difference between any two elements of a sequence
      ),
    control = list(
      verbose = FALSE
      )
    )

#Inspect results CSPADE
summary(subsequences)
as(subsequences, "data.frame")
```

##rule induction:
```{r}
rules <- ruleInduction(frequent_trajectories,
                       confidence = 0, #we need to play with the parameters.
                       control    = list(verbose = TRUE))


pass_rules_df <- as(rules, "data.frame") %>%
  
  separate(rule, into=c("LHS", "RHS"), sep = "=>", remove = T) %>% #remove = F to keep columns
  
#  filter(str_detect(RHS, string_filter)) %>% #FILTER (YES) --> uncomment, (NO)-->comment
  
  arrange(
    LHS, RHS,
    desc(confidence), desc(lift))

```
##calculate appropriate measures:
```{r}
n_students <- length(unique(d_sequence_transactions$`Student ID`))

sequence_rules <- pass_rules_df %>%
  separate(LHS, into=c("LHS Course ID", "LHS Score"), sep="_", remove= FALSE) %>%
  separate(RHS, into=c("RHS Course ID", "RHS Score"), sep="_", remove= FALSE) %>%
  mutate(`RHS Course ID`= substr(`RHS Course ID`, start=4, stop=10))%>%
  left_join(lookup_table, by = c("RHS Course ID"="Course ID")) %>%
  
  # Compute statistics
  group_by(LHS,`RHS Course ID`) %>%
  mutate(Prob_both = sum(support), 
         Conf_corr = support/Prob_both,
         n_rule = support * n_students,
         n_both =Prob_both * n_students,
         lift_diff= Conf_corr - Prop_low,
         lift_corr=Conf_corr / Prop_low) %>%
  ungroup() %>%
  
  # Filter and edit output
  filter(str_detect(`RHS Score`, pattern = "low"),
         str_detect(`LHS Score`, pattern = "low"),
         n_rule >= 5,
         Conf_corr > 0.2) %>%
  select(LHS, RHS, support, n_rule, Prob_both, n_both, Prop_low, Conf_corr, lift_corr, lift_diff) %>%
  mutate(support = round(support,3),
         Prob_both =round(Prob_both,3),
         Prop_low =round(Prop_low,3),
         Conf_corr= round(Conf_corr,3),
         lift_diff= round(lift_diff,3),
         lift_corr= round(lift_corr,3)) %>%
  arrange(lift_diff)

```
##Save data
```{r}
save(sequence_rules,
     file = "sequence_rules.RDATA")
```

##Arrange data for plots:
```{r}
for_plots <-  manual_confidence_rules %>%
  group_by(LHS)%>%
  mutate(LHS_count=n())
```
#Visualizing rules
```{r}
#look at counts
ggplot(for_plots, aes(LHS))+
  geom_bar()

#jitter
ggplot(manual_confidence_rules)+
  geom_jitter(mapping=aes(x=Conf_corr, y=log(support), color=lift_corr))

ggplot(manual_confidence_rules)+
  geom_jitter(mapping=aes(x=log(support), y=lift_corr, color = Conf_corr))

#heatmap
ggplot(select(manual_confidence_rules,LHS,RHS, n_rule), aes(x=RHS,y=LHS, fill=n_rule))+
  geom_raster() +
  theme(
    axis.text.x = element_text(angle=90)
  )

gglot(table(select(manual_confidence_rules,LHS,RHS)))

#ggnetwork

#SEE: http://kateto.net/network-visualization 
g_net <- graph_from_data_frame(d=head(for_plots,50), directed=T) #I BELIEVE THE COLUMNS SHOULD BE REVERSED.

l <- layout_in_circle(g_net)
plot(g_net, #vertex.shape="none",
     vertex.size=.9,
     #vertex.label=NA,
     vertex.label.cex=0.5,
     edge.arrow.size=.1, 
     edge.curved=.1,
     layout=l)

```
#Visualisation
It might be a nice idea to visualise using Sankey diagrams, although I don't know if there will be too many courses for it to be meaningful. We might need to sample. See https://analyzecore.com/2014/10/31/sequence-carts-analysis-sankey/ 

#EXPERIMENTS:
##Condensing grades

```{r}
tmp_distributed_transcript <- d_transcript %>%
  filter(!is.na(Grade), Grade>=0) %>%
  mutate(Round_grade = paste("g", round(Grade,0),sep=""),
         Values = T)
#This is the transcript with cumulative error gap.
distributed_transcript <- spread(tmp_distributed_transcript, key = "Round_grade", value = Values #, fill = F
               )%>%
  mutate(g1  = g0|g1,
         g2  = g1|g2,
         g3  = g2|g3,
         g4  = g3|g4,
         g5  = g4|g5,
         g6  = g5|g6,
         g7  = g6|g7,
         g8  = g7|g8,
         g9  = g8|g9,
         g10 = g9|g10,
         ) %>%
  gather(key="Gr_string", value="Gr_bool", g0, g1, g2, g3, g4,g5,g6,g7,g8,g9,g10, na.rm=T) %>%
  select(-Gr_bool )%>%
  separate(Gr_string, into=c("Trash", "Grade_new"), sep="g")%>%
  select(-Trash) %>%
  mutate(Grade= as.numeric(Grade_new))%>%
  select(-Grade_new)%>%
  filter(!str_detect(Period, " to "))%>% #Crutch FILTER
  mutate(Item=paste(`Course ID`,Grade, sep="_"))%>%
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time))%>%
  select(`Student ID`, Item, Time) %>%
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  group_by(`Student ID`, Time) %>%
  summarize(list_course = list(Item)) %>%
  mutate(TransactionID = paste(`Student ID`, Time, sep = "_"))


  
```

Excluding manatory courses.
```{r}

tmp_distributed_transcript <- d_course %>%
  select(`Course ID`, Type, Letters) %>%
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  inner_join(d_transcript, by = "Course ID") %>%
  filter(!is.na(Grade), Grade>=0) %>%
  mutate(Round_grade = paste("g", round(Grade,0),sep=""),
         Values = T)
#This is the transcript with cumulative error gap.

distributed_transcript <- spread(tmp_distributed_transcript, key = "Round_grade", value = Values #, fill = F
               )%>%
  mutate(g1  = g0|g1,
         g2  = g1|g2,
         g3  = g2|g3,
         g4  = g3|g4,
         g5  = g4|g5,
         g6  = g5|g6,
         g7  = g6|g7,
         g8  = g7|g8,
         g9  = g8|g9,
         g10 = g9|g10,
         ) %>%
  gather(key="Gr_string", value="Gr_bool", g0, g1, g2, g3, g4,g5,g6,g7,g8,g9,g10, na.rm=T) %>%
  select(-Gr_bool )%>%
  separate(Gr_string, into=c("Trash", "Grade_new"), sep="g")%>%
  select(-Trash) %>%
  mutate(Grade= as.numeric(Grade_new))%>%
  select(-Grade_new)%>%
  filter(!str_detect(Period, " to "))%>% #Crutch FILTER
  mutate(Item=paste(`Course ID`,Grade, sep="_"))%>%
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time))%>%
  select(`Student ID`, Item, Time) %>%
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  group_by(`Student ID`, Time) %>%
  summarize(list_course = list(Item)) %>%
  mutate(TransactionID = paste(`Student ID`, Time, sep = "_"))
  
  
```


#puting everything into quasi-function
--------------------------------------------------
##to transaction
```{r}
transaction_list_course <- distributed_transcript$list_course
names(transaction_list_course) <- distributed_transcript$TransactionID

transactions <- as(transaction_list_course, "transactions")
transactionInfo(transactions)$sequenceID <- distributed_transcript$`Student ID`
transactionInfo(transactions)$eventID <- distributed_transcript$Time
transactionInfo(transactions)$transactionID <- NULL

#checking results
as(transactions, "data.frame")
```
##Create filter condition.
```{r}
# string_filter <- case_when(quo_name(category)=="Outcome"        ~ "Fail",
#                            quo_name(category)=="Low_grade"      ~ "low score",
#                            quo_name(category)=="Discrete_grade" ~ "5",
#                            quo_name(category)=="None"           ~ "_" #useless filter
#                            )
```
##CSPADE Algorithm:

```{r}
frequent_trajectories <- cspade(transactions, parameter = list(support = 0.1,
                                                               maxsize = 1,
                                                               maxlen = 2 #,
                                                               #mingap=, #gap between consecutive elements
                                                               #maxgap=, #hap between consecutive elements
                                                               #maxwin=  #gap between ANY two elements
                                                                 ), control   = list(verbose = TRUE))

#Inspect results CSPADE
summary(frequent_trajectories)
as(frequent_trajectories, "data.frame")

```
##rule induction:
```{r}
rules <- ruleInduction(frequent_trajectories,
                       confidence = 0, #we need to play with the parameters.
                       control    = list(verbose = TRUE))


pass_rules_df <- as(rules, "data.frame") %>%
  
  separate(rule, into=c("LHS", "RHS"), sep = "=>", remove = T) %>% #remove = F to keep columns
  
  #filter(str_detect(RHS, string_filter)) %>% #FILTER (YES) --> uncomment, (NO)-->comment
  
  arrange(
    LHS, RHS,
    desc(confidence), desc(lift))

support(frequent_trajectories,transactions, control=list(verbose=T))

```
here I'l playing around with the correct confidence:
#correcting lookup for this case:
First we find out raw proportions per course (how many were low grade/high grade, out of a particular course, and the percentage this represents from the total)
```{r}
lookup_table <- d_course %>%
  
  select(Type, `Course ID`, Letters) %>%
  
  filter(Type == "Elective",
         ! Letters %in% c("SKI", "PRO"),
         ! Letters %in% c("SAS", "SAH", "SAC")) %>%
  
  select(`Course ID`) %>%
  
  left_join(d_transcript, by = "Course ID") %>%
  mutate(
    Outcome = 
           case_when( Grade == -1   ~"Fail", #if we don't do this NG satisfies the condition for pass v
                      Grade  > 5.5 ~"Pass",
                               T~"Fail"),
    Low_grade =
           case_when(Grade == -1 ~"low score", #if we don't do this NG satisfies the condition for pass v
                      Grade > fail_threshold ~"high score",
                               T~"low score"),
    Discrete_grade =
           case_when(Grade <= 5.4             ~ 5.0,
                     between(Grade, 5.5, 6.4) ~ 6.0,
                     between(Grade, 6.5, 7.4) ~ 7.0,
                     between(Grade, 7.5, 8.4) ~ 8.0,
                     between(Grade, 8.5, 9.4) ~ 9.0,
                     Grade >= 9.5             ~ 10.0,
                     T ~ -99999    #this is a control, we should not have any grades here
                     ),
    None= "" # this might not be optimal
         ) %>%
  
  #mutate(Item = paste(`Course ID`, !!category, sep = "_")) %>% #Change for Outcome/Low_grade/Discrete_grade
  
  filter(!str_detect(Period, " to ")) %>% #filter courses that spann more than one period
  
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time)) %>%
  
  select(`Student ID`, #Item,  #************************************ <- 
         `Course ID`, !!category, Time) %>% # Course ID and Time may not be necessary
  
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  
  group_by(`Course ID`, !!category) %>% ##this makes no sense we should just group id and low_grade
  
  summarize(Count = n()) %>%
  #separate(Item, into= c("Course ID", quo_name(category)), sep="_", remove = F) %>%
  ungroup() %>%
  group_by(`Course ID`) %>%
  mutate(Total= sum(Count))%>%
  filter(Low_grade=="low score")%>%
  mutate(Prop_low= Count/Total)
```


```{r}
category
n_students <- length(unique(d_sequence_transactions$`Student ID`))

sequence_rules <- pass_rules_df %>%
  separate(LHS, into=c("LHS Course ID", "LHS Score"), sep="_", remove= FALSE) %>%
  separate(RHS, into=c("RHS Course ID", "RHS Score"), sep="_", remove= FALSE) %>%
  mutate(`RHS Course ID`= substr(`RHS Course ID`, start=4, stop=10))%>% #add more
  left_join(lookup_table, by = c("RHS Course ID"="Course ID")) %>%
  
  # Compute statistics
  group_by(LHS,`RHS Course ID`) %>%
<<<<<<< HEAD
=======
  arrange(LHS,`RHS Course ID`) %>%
>>>>>>> cd3b80a5ba609cf9b69c04544af975694a6bd136
  mutate(
    Prob_both = sum(support))%>%
  ungroup()%>%
  mutate( 
    Conf_corr = support/Prob_both,
    n_rule    = support * n_students,
    n_both    = Prob_both * n_students,
    lift_diff = Conf_corr - Prop_low,
    lift_corr = Conf_corr / Prop_low
    )
  
  # Filter and edit output
  filter(str_detect(`RHS Score`, pattern = "low"),
         str_detect(`LHS Score`, pattern = "low"),
         n_rule >= 5,
         Conf_corr > 0.2) %>%
  select(LHS, RHS, support, n_rule, Prob_both, n_both, Prop_low, Conf_corr, lift_corr, lift_diff) %>%
  mutate(support = round(support,3),
         Prob_both =round(Prob_both,3),
         Prop_low =round(Prop_low,3),
         Conf_corr= round(Conf_corr,3),
         lift_diff= round(lift_diff,3),
         lift_corr= round(lift_corr,3)) %>%
  arrange(lift_diff)

```




---------------------------------------------------

##Puting students into take and not take. 
```{r}
#This df has 731,676 rows:
test <- expand.grid(`Student ID`= unique(d_transcript$`Student ID`),`Course ID`= unique(d_course$`Course ID`)) %>%
  left_join(d_transcript, by=c("Course ID", "Student ID")) %>%
  select(`Course ID`, `Student ID`, Grade, Year_numerical, Period) %>%
  mutate(
    take_course = case_when(
      is.na(Grade)  ~ FALSE,
      T             ~ TRUE
      ),
    PFN = case_when(
      is.na(Grade)  ~ "not",
      Grade >= 5.5  ~ "pass",
      Grade < 5.5   ~ "fail"
      )
    )

table(test$PFN)

```