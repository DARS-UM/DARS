---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---

```{r library, message = F}
library(tidyverse)

library(arulesSequences)
```

# Setup
load previous data and create function to find courses
```{r loading data}
load("Output/data_pillar_1.RDATA")
```

Function that returns title of a course given its code.
```{r function find_course}
find_course <- function(code){ 
  
  dataset <- d_transcript %>%
    filter(`Course ID`== code)
  
  title <- dataset$`Course Title`[1]
  
  return(title)
  
}

# Example
find_course("HUM1005")
```

# Data Exploration

```{r statistic summary}

# For convenience
provide_statistics <- function(data){
  data %>%
    summarise(
      Min       = min(Grade),
      Max       = max(Grade), 
      Mean      = round(mean(Grade), 2), 
      Median    = median(Grade), 
      SD        = round(sd(Grade), 2),
      Fail_rate = round(mean(Fail), 2),
      Fail_n    = sum(Fail),
      n         = n()
      )
}

# Student level
d_transcript %>%
  group_by(`Student ID`) %>%
  provide_statistics()

# Course level
d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  group_by(`Course ID`) %>%
  provide_statistics()

# Cluster level
d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  provide_statistics()

# Concentration evel
d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  gather(X, Concentration, Concentration, `Concentration (additional)`, na.rm = TRUE) %>%
  group_by(Concentration) %>%
  provide_statistics()

# Year level
d_transcript %>%
  group_by(Year_numerical) %>%
  provide_statistics()

# Level level
d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  filter(!is.na(Level)) %>%
  group_by(Level) %>%
  provide_statistics()
```

# Association Rules and Sequence Rules
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

## Creating Transactions
First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 

```{r AR data prep 1}

#
# Threshold: low vs high grade
low_grade <- 6.5


#
# Transactions
d_transactions <- d_course %>%
  
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  select(- Period) %>%
  inner_join(d_transcript, by = "Course ID") %>%
  
  # Identifying sequences
  rename(sequenceID = `Student ID`) %>%
  
  # Identifying time of event (sequence)
  mutate(
    Period  = substr(Period, 1, 1),
    eventID = as.numeric(paste(Year_numerical, Period, sep = ""))
    ) %>%

  # Identifying item
  mutate(
    PF = case_when( Grade <  5.5 ~ "fail",
                    Grade >= 5.5 ~ "pass"),
    HL = case_when( Grade <  low_grade ~ "low",
                    Grade >= low_grade ~ "high"),
    
    item_PF = paste(`Course ID`, PF, sep = "_"),
    item_HL = paste(`Course ID`, HL, sep = "_")
    ) %>%
  rename(
    item = `Course ID`
  )
```

### Transactions for Apriori
```{r AR data prep 2}
transactions <- d_transactions %>%
  group_by(
    sequenceID
    ) %>%
  summarize(
    list_item = list(item)
    ) %>%
  ungroup

transactions <- as(transactions$list_item, "transactions")
```

```{r AR data prep 3 (PF)}
transactions_PF <- d_transactions %>%
  group_by(
    sequenceID
    ) %>%
  summarize(
    list_item = list(item_PF)
    ) %>%
  ungroup

transactions_PF <- as(transactions_PF$list_item, "transactions")
```

```{r AR data prep 4 (HL)}
transactions_HL <- d_transactions %>%
  group_by(
    sequenceID
    ) %>%
  summarize(
    list_item = list(item_HL)
    ) %>%
  ungroup

transactions_HL <- as(transactions_HL$list_item, "transactions")
```


### Sequences for CSPADE
```{r}
sequences_temp <- d_transactions %>%
  group_by(
    sequenceID,
    eventID
    ) %>%
  summarize(
    list_item = list(item)
    ) %>%
  arrange(
    sequenceID,
    eventID
    )

sequences             <- as(sequences_temp$list_item, "transactions")
sequences@itemsetInfo <- as.data.frame(select(sequences_temp, sequenceID, eventID))

#checking results
#inspect(head(sequences, 10))
```

```{r}
sequences_temp <- d_transactions %>%
  group_by(
    sequenceID,
    eventID
    ) %>%
  summarize(
    list_item = list(item_PF)
    ) %>%
  arrange(
    sequenceID,
    eventID
    )

sequences_PF             <- as(sequences_temp$list_item, "transactions")
sequences_PF@itemsetInfo <- as.data.frame(select(sequences_temp, sequenceID, eventID))

#checking results
#inspect(head(sequences_PF, 10))
```

```{r}
sequences_temp <- d_transactions %>%
  group_by(
    sequenceID,
    eventID
    ) %>%
  summarize(
    list_item = list(item_HL)
    ) %>%
  arrange(
    sequenceID,
    eventID
    )

sequences_HL             <- as(sequences_temp$list_item, "transactions")
sequences_HL@itemsetInfo <- as.data.frame(select(sequences_temp, sequenceID, eventID))

#checking results
#inspect(head(sequences_HL, 10))
```


## Apriori Algorithm
```{r}
clean_AR <- function(AR){
  
  AR %>%
    as("data.frame") %>%
    filter(
      count >= 1
    ) %>%
    mutate(
      rules = str_remove(rules, pattern = "_fail"),
      lhs = substr(rules, 2, 8),
      rhs = substr(rules, 15, 21),
      `rhs.support` = confidence / lift
      ) %>%
    mutate_at(
      c("support", "lhs.support", "confidence", "rhs.support", "lift"),
      funs(round(., 5))
      ) %>%
    select(
      lhs, rhs,
      support, count, lhs.support,
      confidence, rhs.support,
      lift
      ) %>%
    arrange(
      desc(count)
    )
  
}
```

We apply Apriori algorithm:
```{r AR apriori}
AR <- apriori(
  data = transactions, 
  parameter = list(
    supp = 0,    # min support
    smax = 1,    # max support
    conf = 0,    # min confidence
    minlen = 2,  # min length of rule
    maxlen = 2,  # max length of rule
    ext = TRUE,
    arem = "diff",
    aval = TRUE,
    minval = 0
    ),
  appearance = NULL,
  control = list(
    verbose = FALSE
    )
  ) %>%
  clean_AR

```

```{r apriori PF}
# creating vector of fail course
course_id_fail <- d_transactions_PF %>%
  filter(PF == "fail") %>%
  distinct(`Course ID`)

# Apply apriori
AR_PF <- apriori(
  data = transactions_PF, 
  parameter = list(
    supp = 0, # min support
    smax = 1,    # max support
    conf = 0,    # min confidence
    minlen = 2,  # min length of rule
    maxlen = 2,  # max length of rule
    ext = TRUE,
    arem = "diff",
    aval = TRUE,
    minval = 0
    ),
  appearance = list(
    both = course_id_fail$`Course ID` # only include rules with fail on LHS and RHS
    ),
  control = list(
    verbose = FALSE
    )
  ) %>%
  clean_AR
```

```{r AR exploration}

# TODO: MAKE APP

data_rules <- AR
data_rules <- AR_PF

# Best support
data_rules %>%
  arrange(desc(support))

# Best confidence
data_rules %>%
  filter(count >= 5) %>%
  arrange(desc(confidence))

# Best lift
data_rules %>%
  filter(count >= 5) %>%
  arrange(desc(lift))
```


## CSPADE Algorithm

```{r}
<<<<<<< HEAD
lookup_table <- d_course %>%
  
  select(Type, `Course ID`, Letters) %>%
  
  filter(Type == "Elective",
         ! Letters %in% c("SKI", "PRO"),
         ! Letters %in% c("SAS", "SAH", "SAC")) %>%
  
  select(`Course ID`) %>%
  
  left_join(d_transcript, by = "Course ID") %>%
  mutate(
    Outcome = 
           case_when( Grade == -1   ~"Fail", #if we don't do this NG satisfies the condition for pass v
                      Grade  > 5.5 ~"Pass",
                               T~"Fail"),
    Low_grade =
           case_when(Grade == -1 ~"low score", #if we don't do this NG satisfies the condition for pass v
                      Grade > fail_threshold ~"high score",
                               T~"low score"),
    Discrete_grade =
           case_when(Grade <= 5.4             ~ 5.0,
                     between(Grade, 5.5, 6.4) ~ 6.0,
                     between(Grade, 6.5, 7.4) ~ 7.0,
                     between(Grade, 7.5, 8.4) ~ 8.0,
                     between(Grade, 8.5, 9.4) ~ 9.0,
                     Grade >= 9.5             ~ 10.0,
                     T ~ -99999    #this is a control, we should not have any grades here
                     ),
    None= "" # this might not be optimal
         ) %>%
  
  #mutate(Item = paste(`Course ID`, !!category, sep = "_")) %>% #Change for Outcome/Low_grade/Discrete_grade
  
  filter(!str_detect(Period, " to ")) %>% #filter courses that spann more than one period
  
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time)) %>%
  
  select(`Student ID`, #Item, 
         `Course ID`, !!category, Time) %>% # Course ID and Time may not be necessary
  
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  
  group_by(`Course ID`, !!category) %>% ##this makes no sense we should just group id and low_grade
  
  summarize(Count = n()) %>%
  #separate(Item, into= c("Course ID", quo_name(category)), sep="_", remove = F) %>%
  ungroup() %>%
  group_by(`Course ID`) %>%
  mutate(Total= sum(Count))%>%
  filter(Low_grade=="low score")%>%
  mutate(Prop_low= Count/Total)
```

###NOTE for FURTURE: create filter by function
##Create filter condition.
```{r}
string_filter <- case_when(quo_name(category)=="Outcome"        ~ "Fail",
                           quo_name(category)=="Low_grade"      ~ "low score",
                           quo_name(category)=="Discrete_grade" ~ "5",
                           quo_name(category)=="None"           ~ "_" #useless filter
                           )
```
##CSPADE Algorithm:
```{r}
frequent_trajectories <- cspade(sequences, parameter = list(support = 0,
                                                               maxsize = 1,
                                                               maxlen = 2), control   = list(verbose = TRUE))
=======
subsequences <- sequences %>%
  cspade(
    parameter = list(
      support = 0, # min support of a sequence
      maxsize = 1, # max number of items of an element of a sequence
      maxlen  = 2, # max number of element of a sequence
      mingap  = 1, # min time difference between consecutive element of a sequence
      maxgap  = 1e4, # max time difference between consecutive element of a sequence
      maxwin  = 1e4  # max time difference between any two elements of a sequence
      ),
    control = list(
      verbose = FALSE
      )
    )
>>>>>>> cd3b80a5ba609cf9b69c04544af975694a6bd136

#Inspect results CSPADE
summary(subsequences)
as(subsequences, "data.frame")
```

##rule induction:
```{r}
rules <- ruleInduction(frequent_trajectories,
                       confidence = 0, #we need to play with the parameters.
                       control    = list(verbose = TRUE))


pass_rules_df <- as(rules, "data.frame") %>%
  
  separate(rule, into=c("LHS", "RHS"), sep = "=>", remove = T) %>% #remove = F to keep columns
  
#  filter(str_detect(RHS, string_filter)) %>% #FILTER (YES) --> uncomment, (NO)-->comment
  
  arrange(
    LHS, RHS,
    desc(confidence), desc(lift))

```
##calculate appropriate measures:
```{r}
n_students <- length(unique(d_sequence_transactions$`Student ID`))

sequence_rules <- pass_rules_df %>%
  separate(LHS, into=c("LHS Course ID", "LHS Score"), sep="_", remove= FALSE) %>%
  separate(RHS, into=c("RHS Course ID", "RHS Score"), sep="_", remove= FALSE) %>%
  mutate(`RHS Course ID`= substr(`RHS Course ID`, start=4, stop=10))%>%
  left_join(lookup_table, by = c("RHS Course ID"="Course ID")) %>%
  
  # Compute statistics
  group_by(LHS,`RHS Course ID`) %>%
  mutate(Prob_both = sum(support), 
         Conf_corr = support/Prob_both,
         n_rule = support * n_students,
         n_both =Prob_both * n_students,
         lift_diff= Conf_corr - Prop_low,
         lift_corr=Conf_corr / Prop_low) %>%
  ungroup() %>%
  
  # Filter and edit output
  filter(str_detect(`RHS Score`, pattern = "low"),
         str_detect(`LHS Score`, pattern = "low"),
         n_rule >= 5,
         Conf_corr > 0.2) %>%
  select(LHS, RHS, support, n_rule, Prob_both, n_both, Prop_low, Conf_corr, lift_corr, lift_diff) %>%
  mutate(support = round(support,3),
         Prob_both =round(Prob_both,3),
         Prop_low =round(Prop_low,3),
         Conf_corr= round(Conf_corr,3),
         lift_diff= round(lift_diff,3),
         lift_corr= round(lift_corr,3)) %>%
  arrange(lift_diff)

```
##Save data
```{r}
save(sequence_rules,
     file = "sequence_rules.RDATA")
```

##Arrange data for plots:
```{r}
for_plots <-  manual_confidence_rules %>%
  group_by(LHS)%>%
  mutate(LHS_count=n())
```
#Visualizing rules
```{r}
#look at counts
ggplot(for_plots, aes(LHS))+
  geom_bar()

#jitter
ggplot(manual_confidence_rules)+
  geom_jitter(mapping=aes(x=Conf_corr, y=log(support), color=lift_corr))

ggplot(manual_confidence_rules)+
  geom_jitter(mapping=aes(x=log(support), y=lift_corr, color = Conf_corr))

#heatmap
ggplot(select(manual_confidence_rules,LHS,RHS, n_rule), aes(x=RHS,y=LHS, fill=n_rule))+
  geom_raster() +
  theme(
    axis.text.x = element_text(angle=90)
  )

gglot(table(select(manual_confidence_rules,LHS,RHS)))

#ggnetwork

#SEE: http://kateto.net/network-visualization 
g_net <- graph_from_data_frame(d=head(for_plots,50), directed=T) #I BELIEVE THE COLUMNS SHOULD BE REVERSED.

l <- layout_in_circle(g_net)
plot(g_net, #vertex.shape="none",
     vertex.size=.9,
     #vertex.label=NA,
     vertex.label.cex=0.5,
     edge.arrow.size=.1, 
     edge.curved=.1,
     layout=l)

```
#Visualisation
It might be a nice idea to visualise using Sankey diagrams, although I don't know if there will be too many courses for it to be meaningful. We might need to sample. See https://analyzecore.com/2014/10/31/sequence-carts-analysis-sankey/ 

#EXPERIMENTS:
##Condensing grades

```{r}
tmp_distributed_transcript <- d_transcript %>%
  filter(!is.na(Grade), Grade>=0) %>%
  mutate(Round_grade = paste("g", round(Grade,0),sep=""),
         Values = T)
#This is the transcript with cumulative error gap.
distributed_transcript <- spread(tmp_distributed_transcript, key = "Round_grade", value = Values #, fill = F
               )%>%
  mutate(g1  = g0|g1,
         g2  = g1|g2,
         g3  = g2|g3,
         g4  = g3|g4,
         g5  = g4|g5,
         g6  = g5|g6,
         g7  = g6|g7,
         g8  = g7|g8,
         g9  = g8|g9,
         g10 = g9|g10,
         ) %>%
  gather(key="Gr_string", value="Gr_bool", g0, g1, g2, g3, g4,g5,g6,g7,g8,g9,g10, na.rm=T) %>%
  select(-Gr_bool )%>%
  separate(Gr_string, into=c("Trash", "Grade_new"), sep="g")%>%
  select(-Trash) %>%
  mutate(Grade= as.numeric(Grade_new))%>%
  select(-Grade_new)%>%
  filter(!str_detect(Period, " to "))%>% #Crutch FILTER
  mutate(Item=paste(`Course ID`,Grade, sep="_"))%>%
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time))%>%
  select(`Student ID`, Item, Time) %>%
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  group_by(`Student ID`, Time) %>%
  summarize(list_course = list(Item)) %>%
  mutate(TransactionID = paste(`Student ID`, Time, sep = "_"))


  
```

Excluding manatory courses.
```{r}

tmp_distributed_transcript <- d_course %>%
  select(`Course ID`, Type, Letters) %>%
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  inner_join(d_transcript, by = "Course ID") %>%
  filter(!is.na(Grade), Grade>=0) %>%
  mutate(Round_grade = paste("g", round(Grade,0),sep=""),
         Values = T)
#This is the transcript with cumulative error gap.

distributed_transcript <- spread(tmp_distributed_transcript, key = "Round_grade", value = Values #, fill = F
               )%>%
  mutate(g1  = g0|g1,
         g2  = g1|g2,
         g3  = g2|g3,
         g4  = g3|g4,
         g5  = g4|g5,
         g6  = g5|g6,
         g7  = g6|g7,
         g8  = g7|g8,
         g9  = g8|g9,
         g10 = g9|g10,
         ) %>%
  gather(key="Gr_string", value="Gr_bool", g0, g1, g2, g3, g4,g5,g6,g7,g8,g9,g10, na.rm=T) %>%
  select(-Gr_bool )%>%
  separate(Gr_string, into=c("Trash", "Grade_new"), sep="g")%>%
  select(-Trash) %>%
  mutate(Grade= as.numeric(Grade_new))%>%
  select(-Grade_new)%>%
  filter(!str_detect(Period, " to "))%>% #Crutch FILTER
  mutate(Item=paste(`Course ID`,Grade, sep="_"))%>%
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time))%>%
  select(`Student ID`, Item, Time) %>%
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  group_by(`Student ID`, Time) %>%
  summarize(list_course = list(Item)) %>%
  mutate(TransactionID = paste(`Student ID`, Time, sep = "_"))
  
  
```


#puting everything into quasi-function
--------------------------------------------------
##to transaction
```{r}
transaction_list_course <- distributed_transcript$list_course
names(transaction_list_course) <- distributed_transcript$TransactionID

transactions <- as(transaction_list_course, "transactions")
transactionInfo(transactions)$sequenceID <- distributed_transcript$`Student ID`
transactionInfo(transactions)$eventID <- distributed_transcript$Time
transactionInfo(transactions)$transactionID <- NULL

#checking results
as(transactions, "data.frame")
```
##Create filter condition.
```{r}
# string_filter <- case_when(quo_name(category)=="Outcome"        ~ "Fail",
#                            quo_name(category)=="Low_grade"      ~ "low score",
#                            quo_name(category)=="Discrete_grade" ~ "5",
#                            quo_name(category)=="None"           ~ "_" #useless filter
#                            )
```
##CSPADE Algorithm:

```{r}
frequent_trajectories <- cspade(transactions, parameter = list(support = 0.1,
                                                               maxsize = 1,
                                                               maxlen = 2 #,
                                                               #mingap=, #gap between consecutive elements
                                                               #maxgap=, #hap between consecutive elements
                                                               #maxwin=  #gap between ANY two elements
                                                                 ), control   = list(verbose = TRUE))

#Inspect results CSPADE
summary(frequent_trajectories)
as(frequent_trajectories, "data.frame")

```
##rule induction:
```{r}
rules <- ruleInduction(frequent_trajectories,
                       confidence = 0, #we need to play with the parameters.
                       control    = list(verbose = TRUE))


pass_rules_df <- as(rules, "data.frame") %>%
  
  separate(rule, into=c("LHS", "RHS"), sep = "=>", remove = T) %>% #remove = F to keep columns
  
  #filter(str_detect(RHS, string_filter)) %>% #FILTER (YES) --> uncomment, (NO)-->comment
  
  arrange(
    LHS, RHS,
    desc(confidence), desc(lift))

support(frequent_trajectories,transactions, control=list(verbose=T))

```
here I'l playing around with the correct confidence:
#correcting lookup for this case:
First we find out raw proportions per course (how many were low grade/high grade, out of a particular course, and the percentage this represents from the total)
```{r}
lookup_table <- d_course %>%
  
  select(Type, `Course ID`, Letters) %>%
  
  filter(Type == "Elective",
         ! Letters %in% c("SKI", "PRO"),
         ! Letters %in% c("SAS", "SAH", "SAC")) %>%
  
  select(`Course ID`) %>%
  
  left_join(d_transcript, by = "Course ID") %>%
  mutate(
    Outcome = 
           case_when( Grade == -1   ~"Fail", #if we don't do this NG satisfies the condition for pass v
                      Grade  > 5.5 ~"Pass",
                               T~"Fail"),
    Low_grade =
           case_when(Grade == -1 ~"low score", #if we don't do this NG satisfies the condition for pass v
                      Grade > fail_threshold ~"high score",
                               T~"low score"),
    Discrete_grade =
           case_when(Grade <= 5.4             ~ 5.0,
                     between(Grade, 5.5, 6.4) ~ 6.0,
                     between(Grade, 6.5, 7.4) ~ 7.0,
                     between(Grade, 7.5, 8.4) ~ 8.0,
                     between(Grade, 8.5, 9.4) ~ 9.0,
                     Grade >= 9.5             ~ 10.0,
                     T ~ -99999    #this is a control, we should not have any grades here
                     ),
    None= "" # this might not be optimal
         ) %>%
  
  #mutate(Item = paste(`Course ID`, !!category, sep = "_")) %>% #Change for Outcome/Low_grade/Discrete_grade
  
  filter(!str_detect(Period, " to ")) %>% #filter courses that spann more than one period
  
  mutate( Time = paste(Year_numerical, Period, sep = ""),
          Time = as.double(Time)) %>%
  
  select(`Student ID`, #Item,  #************************************ <- 
         `Course ID`, !!category, Time) %>% # Course ID and Time may not be necessary
  
  arrange(`Student ID`, Time) %>% #Note that integer identifiers must be positive and that transactions must be ordered by sequenceID and eventID.
  
  group_by(`Course ID`, !!category) %>% ##this makes no sense we should just group id and low_grade
  
  summarize(Count = n()) %>%
  #separate(Item, into= c("Course ID", quo_name(category)), sep="_", remove = F) %>%
  ungroup() %>%
  group_by(`Course ID`) %>%
  mutate(Total= sum(Count))%>%
  filter(Low_grade=="low score")%>%
  mutate(Prop_low= Count/Total)
```


```{r}
category
n_students <- length(unique(d_sequence_transactions$`Student ID`))

sequence_rules <- pass_rules_df %>%
  separate(LHS, into=c("LHS Course ID", "LHS Score"), sep="_", remove= FALSE) %>%
  separate(RHS, into=c("RHS Course ID", "RHS Score"), sep="_", remove= FALSE) %>%
  mutate(`RHS Course ID`= substr(`RHS Course ID`, start=4, stop=10))%>% #add more
  left_join(lookup_table, by = c("RHS Course ID"="Course ID")) %>%
  
  # Compute statistics
  group_by(LHS,`RHS Course ID`) %>%
<<<<<<< HEAD
=======
  arrange(LHS,`RHS Course ID`) %>%
>>>>>>> cd3b80a5ba609cf9b69c04544af975694a6bd136
  mutate(
    Prob_both = sum(support))%>%
  ungroup()%>%
  mutate( 
    Conf_corr = support/Prob_both,
    n_rule    = support * n_students,
    n_both    = Prob_both * n_students,
    lift_diff = Conf_corr - Prop_low,
    lift_corr = Conf_corr / Prop_low
    )
  
  # Filter and edit output
  filter(str_detect(`RHS Score`, pattern = "low"),
         str_detect(`LHS Score`, pattern = "low"),
         n_rule >= 5,
         Conf_corr > 0.2) %>%
  select(LHS, RHS, support, n_rule, Prob_both, n_both, Prop_low, Conf_corr, lift_corr, lift_diff) %>%
  mutate(support = round(support,3),
         Prob_both =round(Prob_both,3),
         Prop_low =round(Prop_low,3),
         Conf_corr= round(Conf_corr,3),
         lift_diff= round(lift_diff,3),
         lift_corr= round(lift_corr,3)) %>%
  arrange(lift_diff)

```




---------------------------------------------------

##Puting students into take and not take. 
```{r}
#This df has 731,676 rows:
test <- expand.grid(`Student ID`= unique(d_transcript$`Student ID`),`Course ID`= unique(d_course$`Course ID`)) %>%
  left_join(d_transcript, by=c("Course ID", "Student ID")) %>%
  select(`Course ID`, `Student ID`, Grade, Year_numerical, Period) %>%
  mutate(
    take_course = case_when(
      is.na(Grade)  ~ FALSE,
      T             ~ TRUE
      ),
    PFN = case_when(
      is.na(Grade)  ~ "not",
      Grade >= 5.5  ~ "pass",
      Grade < 5.5   ~ "fail"
      )
    )

table(test$PFN)

```