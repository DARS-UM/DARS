---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache.path = "Cache/Pillar 1/")

library(tidyverse)
library(arulesSequences)
```

# Set up
We load the environment `data_pillar1` which we saved at the end of the data preparation. It contains the data sets `d_course` and `d_transcript`.
```{r loading data}
load("Output/data_general.RDATA")
load("Output/data_pillar_1.RDATA")
```

```{r constants}
# Some old courses are present
course_all  <- unique(d_transcript$`Course ID` )
student_all <- unique(d_transcript$`Student ID`)
n_students  <- length(student_all)
```

```{r helper function}
# helper function
join_d_course     <- function(df) df %>% left_join(d_course    , by = c("Course ID")) # left_join to keep all courses present in d_transcript
join_d_transcript <- function(df) df %>% right_join(d_transcript, by = c("Course ID")) # right_joint to keep all courses present in d_transcript

paste_ <- function(...) paste(..., sep = "_")
```

# Descriptive statistics
We compute summary statistics (minimum, maximum, mean, median, standard deviation, failure rate, number of failure and count) at different levels (student, course, cluster, concentration, year and course level). We save the results in the environment `Transcript Statistics`.

```{r statistics set up}
pass_grade <- 5.5
high_grade <- 6.5
```

```{r provide_statistics}
# helper function
provide_statistics <- function(var, df = d_transcript, high_gr = high_grade, pass_gr = pass_grade, join = TRUE){
  
  if(join) df <- df %>% join_d_course
  
  df %>%
    
    filter(!is.na(!!enquo(var))) %>%
    
    group_by(!!enquo(var)) %>%
    
    summarise(
      Count           = n(),
      Min             = min   (Grade),
      Max             = max   (Grade),
      Mean            = mean  (Grade),
      Median          = median(Grade),
      IQR             = IQR   (Grade),
      SD              = sd    (Grade),
      `Low Rate`      = mean  (Grade < high_grade),
      `Fail Rate`     = mean  (Grade < pass_grade)
    ) %>%
    
    mutate_if(is.numeric, round, digits = 2)
  
}
```

```{r statistics}
statistics <- list()

# Student level
statistics$student <- provide_statistics(`Student ID`)

# Course level
statistics$course  <- provide_statistics(`Course ID`)

# Cluster level
statistics$cluster <- provide_statistics(Cluster)

# Type level
statistics$type    <- provide_statistics(Type)

# Concentration level
statistics$concentration <- d_transcript %>%
  join_d_course %>%
  gather(
    key = X, 
    value = Concentration, 
    Concentration, `Concentration (additional)`,
    na.rm = TRUE
    ) %>%
  provide_statistics(Concentration, df = ., join = FALSE)

# Year level
statistics$year <- provide_statistics(`Academic Year`)

# Level level
statistics$level <- provide_statistics(Level) # TODO: filter for student who completed their studies
```

```{r save statistics, include = FALSE}
save(statistics, file = "Output/Transcript Statistics.RDATA")
rm(statistics, provide_statistics)
```

# Creating Transactions and Sequences
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 
## Data Preparation

### Take, fail, low grade
```{r d_transaction taken_PF_HL}
d_transactions <- list()

d_transactions$taken_PF_HL <- d_course %>%
  
  # Exclude courses taken by majority of students (uninformative)
  filter(
    Type != "Mandatory",
    ! Letters %in% c("SKI", "PRO", "SAS", "SAH", "SAC")
    ) %>%
  
  # Join with transcripts
  select(- Period) %>% # remove Period to avoid duplicates
  join_d_transcript %>%
  
  # sequenceID and event ID
  rename(
    sequenceID = `Student ID`,
    eventID = time
    ) %>%

  # itemID
  mutate(
    item    = `Course ID`,
    item_PF = if_else(Grade >= pass_grade, paste_(item, "pass"), paste_(item, "fail")),
    item_PF = if_else(Grade >= high_grade, paste_(item, "high"), paste_(item, "low" ))
    )
```

### Take / not take, pass / fail / not take
```{r d_transactions T_PF_HL}
d_transactions$T_PF_HL <- expand.grid(
  
  # Expand along students (sequenceID) and courses (itemID)
  sequenceID       = student_all,
  item             = course_all,
  stringsAsFactors = FALSE
  ) %>%
  
  # Join with d_transactions
  left_join(
    d_transactions$taken_PF_HL, 
    by = c("sequenceID", "item")
    ) %>%
  
  # Create item
  mutate(
    
    TPF = case_when(
      is.na(Grade)        ~ "not",
      Grade <  pass_grade ~ "fail",
      Grade >= pass_grade ~ "pass"
      ),
    THL = case_when(
      is.na(Grade)        ~ "not",
      Grade <  high_grade ~ "low",
      Grade >= high_grade ~ "high"
      ),

    item_TPF = paste(item, TPF, sep = "_"),
    
    item_THL = paste(item, THL, sep = "_")
    
    )
```

### Grade
```{r transactions grade}
d_transactions$G <- d_transactions$taken_PF_HL %>%
  
  # Spread along ceiling grades
  mutate(
    grade_ceil = ceiling(Grade),
    Values     = TRUE,
    id         = 1 : n() # to ensure each row is unique
    ) %>%
  spread(
    key   = grade_ceil, 
    value = Values
    ) %>%
  
  # grade x or lower
  mutate(
    `1`  = `0`|`1`,
    `2`  = `1`|`2`,
    `3`  = `2`|`3`,
    `4`  = `3`|`4`,
    `5`  = `4`|`5`,
    `6`  = `5`|`6`,
    `7`  = `6`|`7`,
    `8`  = `7`|`8`,
    `9`  = `8`|`9`,
    `10` = `9`|`10`
    ) %>%
  
  # Gather along rounded grades
  gather(
    key   = grade_ceil,
    value = X,
    `0` :`10`, 
    na.rm   = TRUE, 
    convert = TRUE
    ) %>%
  select(
    -X
    ) %>%

  # Identifying item
  mutate(
    item_G = paste(item, grade_ceil, sep = "_")
    )
```
####Save d_transactions
```{r save d_transaction}
save(
  d_transactions,
  file = "Output/transaction_df.RDATA"
  )
```
### Transcript with preceding courses (for SR by hand)
```{r d_transcript_cum, cache = TRUE, warning = FALSE}
d_transcript_cum <- d_transactions$taken_PF_HL %>%
  
  # item_take
  mutate(
    item_take = paste(item, "take", sep = "_")
  ) %>%
  
  # course current
  group_by(
    sequenceID,
    eventID
    ) %>%
  summarize_at(
    c("item", "item_take", "item_PF", "item_HL"),
    function(x) list(unique(x))
    ) %>%
  
  # course current all
  rowwise %>%
  mutate(
    course_current = list(c(item, item_take, item_PF, item_HL))
    ) %>%
  
  # course past
  group_by(
    sequenceID
    ) %>%
  mutate(
    course_past = Reduce(
      f = c,
      x = course_current,
      accumulate = TRUE
      ) %>%
      lag(
        default = as.character(NA)
        )
    ) %>%
  
  # course so far, course not so far
  rowwise %>%
  mutate(
    
    course_past_current = list(union  (course_past, course_current      )),
    
    course_not_yet      = list(setdiff(course_all , course_past_current )),
    course_not_yet      = list(paste(course_not_yet, "not", sep = "_"   )),
    
    course_so_far       = list(union  (course_past, course_not_yet))
    
    ) %>%
  ungroup %>%
  
  # lhs: past, rhs: present
  select(
    lhs = course_so_far,
    rhs = course_current
  )
```

## Transactions
```{r making transactions, cache = TRUE}
make_transaction <- function(data = d_transactions$taken_PF_HL, item = item){
  
  data %>%
    group_by(
      sequenceID
    ) %>%
    summarise(
      list_item = list(unique(!!enquo(item)))
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
}


#
# Making transactions
transactions <- list()

transactions$taken <- make_transaction(item = item   )
transactions$PF    <- make_transaction(item = item_PF)
transactions$HL    <- make_transaction(item = item_HL)
transactions$TPF   <- make_transaction(data = d_transactions$T_PF_HL, item = item_TPF)
transactions$THL   <- make_transaction(data = d_transactions$T_PF_HL, item = item_THL)
transactions$G     <- make_transaction(data = d_transactions$G  , item = item_G  )

# inspect(head(transactions$taken, 10))
```


## Sequences
```{r making sequences, cache = TRUE}
make_sequence <- function(data = d_transactions$taken_PF_HL, item = item){
  
  data_temp <- data %>%
    arrange(
      sequenceID,
      eventID
    ) %>%
    filter(
      !is.na(eventID) # exclude courses that were never taken.
      ) %>% 
    
    group_by(
      sequenceID,
      eventID
    ) %>%
    summarise(
      list_item = list(unique(!!enquo(item)))
    ) %>%
    ungroup
  
  sequences <- data_temp %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
  # indicate sequence ID and event ID for each transaction
  sequences@itemsetInfo <- data_temp %>%
    select(
      sequenceID,
      eventID
      ) %>%
    as.data.frame
  
  return(sequences)
  
}


#
# Making sequences
sequences <- list()

sequences$taken <- make_sequence(item = item   )
sequences$PF    <- make_sequence(item = item_PF)
sequences$HL    <- make_sequence(item = item_HL)
sequences$G     <- make_sequence(data = d_transactions$G    , item = item_G  )

#inspect(head(sequences$G, 10))
```

## Support
```{r}
d_support <- list()

# Probability of taking a course
d_support$TNT <- d_transactions$taken_PF_HL %>%
  distinct(
    item,
    sequenceID
    ) %>%
  count(
    item
    ) %>%
  mutate(
    rate.take = n / n_students,
    rate.not  = 1 - rate.take
    )


#
# Probability of failing, having a low grade
d_support$PF_HL <- d_transactions$taken_PF_HL %>%
  group_by(
    item
    ) %>%
  summarise(
    rate.fail = mean(PF == "fail"),
    rate.low  = mean(HL == "low")
    )


#
# Probability oh having a grade equal or lower than x
d_support$G <- d_transactions$taken_PF_HL %>%
  group_by(
    item
    ) %>%
  summarise(
    `0`    = mean(Grade <= 0),
    `1`    = mean(Grade <= 1),
    `2`    = mean(Grade <= 2),
    `3`    = mean(Grade <= 3),
    `4`    = mean(Grade <= 4),
    `5`    = mean(Grade <= 5),
    `6`    = mean(Grade <= 6),
    `7`    = mean(Grade <= 7),
    `8`    = mean(Grade <= 8),
    `9`    = mean(Grade <= 9),
    `10`   = mean(Grade <= 10)
    ) %>%
  gather(
    key   = grade_ceil,
    value = support.grade,
    `0` : `10`,
    convert = TRUE
    )
```

```{r, include = FALSE}
save(
  transactions, sequences,
  d_transcript_cum,
  d_support, n_students,
  file = "Output/Transaction and Sequences.RDATA"
  )
```

```{r, include = FALSE}
rm(
  
  high_grade, pass_grade, 
  course_all, student_all, n_students,
  
  d_transactions, d_support, d_transcript_cum,
  
  make_transaction, transactions, 
  make_sequence, sequences
  
  )
```


# Mining Rules

```{r}
load("Output/Transaction and Sequences.RDATA")
```

## Functions

### my_aprior, my_cspade and my_ruleInduction

The function `my_apriori()` applies the apriori algorithm on a set of transactions with the parameters that we have chosen.
```{r my_apriori}
my_apriori <- function(data){
  
  data %>%
    
    apriori(
      
      # include all AR possible
      parameter = list(
        supp   = 0, # min support
        smax   = 1, # max support
        conf   = 0, # min confidence
        minlen = 2, # min length of rule
        maxlen = 2  # max length of rule
        ),
      
      # no printing during execution
      control = list(
        verbose = FALSE
        )
      
      )
  
}
```

The function `my_cspade()` applies the cspade algorithm on a set of sequential transactions with the parameters that we have chosen.
```{r my_cspade function}
my_cspade <- function(data){
  
  data %>%
    
    cspade(
      
      parameter = list(
        
        # only include subsequences of the type {one item} => {one item}
        maxlen  = 2,   # max length of sequence
        maxsize = 1,   # max number item for element
        
        # include all such subsequences
        support = 0,   # min suppor
        mingap  = 1,   # min time difference between consecutive element
        maxgap  = 1e4  # max time difference between consecutive element
        
        ),
      
      control = list(
        verbose = FALSE
        )
      
      )
  
}
```

The function `my_ruleInduction()` creates rules from the set of frequent sequences detemined by cspade.
```{r my_ruleInduction}
my_ruleInduction <- function(rules){
  
  rules %>%
    
    ruleInduction(
      
      # keep all subsequences
      confidence = 0,
      
      control    = list(verbose = FALSE)
      
      )
  
}
```

### clean_rules
The function `clean_AR()` transforms the rules generated by the function `my_apriori()` into a readable dataframe
```{r function clean_rules}
clean_rules <- function(rules){
  
  rules %>%
    
    as("data.frame") %>%
    
    # Exclude rules with no support
    filter(
      support > 0
      ) %>%
    
    # separate rule
    rename_at(
      vars(matches("rule")),
      function(x) substr(x, 1, 4)
    ) %>%
    mutate(
      rule = str_remove_all(rule, pattern = "[<]|[{]|[}]|[>]")
      ) %>%
    separate(
      rule,
      into = c("lhs", "rhs"),
      sep = " = "
      ) %>%
    
    # separate lhs and rhs
    separate(
      lhs,
      into    = c("lhs_course", "lhs_outcome"),
      sep     = "_",
      remove  = FALSE,
      convert = TRUE
      ) %>%
    separate(
      rhs,
      into    = c("rhs_course", "rhs_outcome"),
      sep     = "_",
      remove  = FALSE,
      convert = TRUE
      )
  
}
```

### compute_rhs.support
The function `compute_support()` computes the support of the lhs and rhs of the rules.
```{r compute_support}
compute_rhs.support <- function(AR, data_support, type_rule){

  AR %>%
    
    left_join(
      select(data_support, item, !!enquo(type_rule)),
      by = c("rhs_course" = "item")
      ) %>%
    mutate(
      rhs.support = !!enquo(type_rule)
      )

}
```

### compute_conf_lift

```{r}
compute_conf_lift <- function(rules){
  
  rules %>%
  
  # support (and count) of doing lhs and taking course of rhs
  group_by(
    lhs,
    rhs_course
    ) %>%
    mutate(
      lhs.rhsTake.support = sum(support),
      lhs.rhsTake.count   = lhs.rhsTake.support * n_students
      ) %>%
    ungroup %>%
    
    mutate(
      confidence = support / lhs.rhsTake.support,
      lift       = confidence / rhs.support
      )
  
}
```


### make_AR and  make_SR (wrapers)

We encapsulate the functions `my_apriori()`, `clean_AR()` and `compute_support()` into the function `make_AR()`.
```{r make_AR}
make_AR <- function(data, data_support, type_rule){
  
  data %>%
    my_apriori %>%
    clean_rules %>%
    compute_rhs.support(
      data_support = data_support,
      type_rule = !!enquo(type_rule)
      )
  
}
```

We encapsulate the fuctions `my_cpade`, `my_ruleInduction`, and `clean_rules` into `make_SR`
```{r make_SR}
make_SR <- function(data, data_support, type_rule){
  
  data %>%
    my_cspade %>%
    my_ruleInduction %>%
    clean_rules %>%
    compute_rhs.support(
      data_support = data_support,
      type_rule = !!enquo(type_rule)
      )
  
}
```

## Association Rule
We apply make_AR to get different sets of rules:

### take -> take
```{r AR take, cache = TRUE, warning = FALSE}
AR <- list()

AR$taken <- transactions$taken %>%
  make_AR(
    data_support = d_support$TNT, 
    type_rule = rate.take
    )
```

### fail -> fail
```{r AR fail, cache = TRUE}
AR$PF <- transactions$PF %>%
  
  make_AR(
    data_support = d_support$PF_HL,
    type_rule = rate.fail
    ) %>%
  
  compute_conf_lift %>%
  
  # lhs and rhs must be fail
  filter(
    str_detect(lhs, "fail"),
    str_detect(rhs, "fail")
    )
```

### low grade -> low grade
```{r AR low, cache = TRUE}
AR$HL <- transactions$HL %>%
  
  make_AR(
    data_support = d_support$PF_HL,
    type_rule = rate.low
    ) %>%
  
  compute_conf_lift %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "low"),
    str_detect(rhs, "low")
    )
```

### not -> fail
```{r AR TPF, cache = TRUE}
AR$TPF <- transactions$TPF %>%
  
  make_AR(
    data_support = d_support$PF_HL, 
    type_rule = rate.fail
    ) %>%
  
  # Confidence and Lift
  filter(
    str_detect(lhs, "not"),
    str_detect(rhs, "fail|pass") # only keep rhsTake for computing confidence and lift
    ) %>%
  compute_conf_lift %>%
  
  # rhs must be fail, count >= 5
  filter(
    str_detect(rhs, "fail")
    )
```

### not -> low
```{r AR THL, cache = TRUE}
AR$THL <- transactions$THL %>%
  
  make_AR(
    data_support = d_support$PF_HL, 
    type_rule = rate.low
    ) %>%
  
  # only keep lhs is not and rhs is low or high to compute Confidence and Lift
  filter(
    str_detect(lhs, "not"),
    str_detect(rhs, "low|high")
    ) %>%
  compute_conf_lift %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(rhs, "low")
    )
```

### grade less than or equal to x -> grade less than or equal to 6

```{r AR grade, cache = TRUE}
AR$G <- transactions$G %>%
  
  my_apriori %>%
  
  clean_rules %>%
  
  # rhs.support
  left_join(
    d_support$G,
    by = c("rhs_course" = "item", "rhs_outcome" = "grade_ceil")
    ) %>%
  mutate(
    rhs.support = support.grade
    ) %>%
  
  # Confidence and lift
  group_by(
    lhs, 
    rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = max(support),
    lhs.rhsTake.count   = lhs.rhsTake.support * n_students
    ) %>%
  ungroup %>%
  
  mutate(
    confidence = support / lhs.rhsTake.support,
    lift       = confidence / rhs.support
    ) %>%
  
  # Reduce number of rules
  filter(
    # exclude rules with same course on lhs and rhs
    lhs_course != rhs_course,
    # rhs is grade less than or equal to 6
    rhs_outcome <= 6,
    lhs_outcome <= 7
    )
```

## Sequence Rules
We apply make_SR to get different types of sequential rules:

### take -> take
```{r SR taken, cache = TRUE, warning = FALSE}
SR <- list()

SR$taken <- sequences$taken %>%
  
  make_SR(
    data_support = d_support$TNT, 
    type_rule = rate.take
  )
```

### fail -> fail
```{r SR PF, cache = TRUE}
SR$PF <- sequences$PF %>%
  
  make_SR(
    data_support = d_support$PF_HL,
    type_rule = rate.fail
  ) %>%
  
  compute_conf_lift %>%
  
  # lhs and rhs are fail
  filter(
    str_detect(lhs, "fail"),
    str_detect(rhs, "fail")
    )
```

### low grade -> low grade
```{r SR HL, cache = TRUE}
SR$HL <- sequences$HL %>%
  
  make_SR(
    data_support = d_support$PF_HL,
    type_rule = rate.low
  ) %>%
  
  compute_conf_lift %>%
  
  # lhs and rhs are low
  filter(
    str_detect(lhs, "low"),
    str_detect(rhs, "low")
    )
```

### not -> fail
```{r SR TPF, cache = TRUE}
SR$TPF <- d_transcript_cum %>%
  
  # rhs
  unnest(
    rhs,
    .drop = FALSE
    ) %>%
  filter(
    str_detect(rhs, "pass|fail")
    ) %>%
  
  # lhs
  unnest(
    lhs,
    .drop = FALSE
    ) %>%
  filter(
    str_detect(lhs, "not")
    ) %>%
  
  # rule
  unite(
    col = "rule",
    lhs, rhs,
    sep = " => "
    ) %>%
  
  # rule support
  count(
    rule
    ) %>%
  mutate(
    support = n / n_students
    ) %>%
  
  # regular fucntions
  clean_rules %>%
  
  compute_rhs.support(
    data_support = d_support$PF_HL,
    type_rule    = rate.fail
    ) %>%
  
  compute_conf_lift %>%
  
  # lhs not, rhs low
  filter(
    str_detect(rhs, "fail")
    )
```

### not -> low
```{r SR THL, cache = TRUE}
SR$THL <- d_transcript_cum %>%
  
  # rhs
  unnest(
    rhs,
    .drop = FALSE
    ) %>%
  filter(
    str_detect(rhs, "high|low")
    ) %>%
  
  # lhs
  unnest(
    lhs,
    .drop = FALSE
    ) %>%
  filter(
    str_detect(lhs, "not")
    ) %>%
  
  # rule
  unite(
    col = "rule",
    lhs, rhs,
    sep = " => "
    ) %>%
  
  # rule support
  count(
    rule
    ) %>%
  mutate(
    support = n / n_students
    ) %>%
  
  # regular fucntions
  clean_rules %>%
  
  compute_rhs.support(
    data_support = d_support$PF_HL,
    type_rule    = rate.low
    ) %>%
  
  compute_conf_lift %>%
  
  # constraints to respect
  filter(
    str_detect(rhs, "low")
    )
```

### grade less than or equal to x -> grade less than or equal to 6
```{r SR G, cache = TRUE}
SR$G <- sequences$G %>%
  
  my_cspade %>%
  my_ruleInduction %>%
  clean_rules %>%
  
  # rhs.support
  left_join(
    d_support$G,
    by = c("rhs_course" = "item", "rhs_outcome" = "grade_ceil")
    ) %>%
  mutate(
    rhs.support = support.grade
    ) %>%
  
  # Confidence & lift
  group_by(
    lhs,
    rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = max(support),
    lhs.rhsTake.count   = lhs.rhsTake.support * n_students
    ) %>%
  ungroup %>%
  
  mutate(
    confidence = support / lhs.rhsTake.support,
    lift       = confidence / rhs.support
    ) %>%
  
  # Reduce number of rules
  filter(
    # exclude rules with same course on lhs and rhs
    lhs_course != rhs_course,
    # rhs is grade less than or equal to 6
    rhs_outcome <= 6,
    lhs_outcome <= 7
    )
```

## Editing rules
The function edit_rules makes AR easier to read. It keeps only rules that appear more than 5 times, rounds numerical variables to 5 significant digits, and drops aiding columns which were only used for computation in previous stages but add no additional information.
```{r edit_rules}
edit_rules_rulesApp <- function(rules){
  
  rules %>%
    
    mutate(
      count = support * n_students
      ) %>%
    
    select(
      lhs, rhs,
      matches("rhsTake.count"), count,
      matches("rhsTake.support"),
      confidence,
      support, 
      lift
      ) %>%
    
     mutate_if(
       is.numeric,
       round,
       digits = 3
       ) %>%
    
    arrange(
      desc(count)
      )
  
}
```

```{r edit rules}
AR_rulesAPP <- lapply(
  X   = AR,
  FUN = edit_rules_rulesApp
  )

SR_rulesAPP <- lapply(
  X   = SR,
  FUN = edit_rules_rulesApp
  )
```

```{r}
save(
  AR_rulesAPP, SR_rulesAPP,
  file = "App/rules/rules.RDATA"
  )
```

```{r show rules}
print(AR_rulesAPP$THL)
print(SR_rulesAPP$THL)
```

```{r}
edit_rules_RSAPP <- function(rules){
  
  rules %>%
    
    mutate(
      count = support * n_students
      ) %>%
    
    filter(
       count      >= 10,
       lift       >= 1,
       confidence >= 0.4
      )
  
}
```

```{r}
AR_RSAPP <- lapply(
  X   = AR,
  FUN = edit_rules_RSAPP
  )

SR_RSAPP <- lapply(
  X   = SR,
  FUN = edit_rules_RSAPP
  )
```


```{r save rules, include = FALSE}
save(
  AR_RSAPP, SR_RSAPP,
  file = "App/Recommender System/rules.RDATA"
  )
```