---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---

```{r TODO}
# TODO: compute corrected confidence for AR
# TODO: compute lhs.support, for grade
```

```{r library, message = FALSE, warning = FALSE}
library(tidyverse)
library(arulesSequences)
```

# Setup
We load the environment `data_pillar1` which we saved at the end of the data preparation. It contains the data sets `d_course` and `d_transcript`.
```{r loading data}
load("Output/data_pillar_1.RDATA")
```

We create a function, which, when given the code of a course, returns its title.
```{r function find_course}
find_course <- function(code){ 
  
  dataset <- d_transcript %>%
    filter(`Course ID`== code)
  
  title <- dataset$`Course Title`[1]
  
  return(title)
  
}

# Example
find_course("HUM1005")
```

# Data Exploration

We compute summary statistics (minimum, maximum, mean, median, standard deviation, failure rate, number of failure and count) at different levels (student, course, cluster, concentration, year and course level). We save the results in the environment `Transcript Statistics`.

```{r statistic summary}

# For convenience
provide_statistics <- function(data){
  
  data %>%
    summarise(
      Min       = min(Grade),
      Max       = max(Grade), 
      Mean      = mean(Grade), 
      Median    = median(Grade), 
      SD        = sd(Grade),
      `Failure Rate` = mean(Fail),
      `Failure Count`    = sum(Fail),
      Count     = n()
      ) %>%
    mutate_at(
      vars(Mean, SD, `Failure Rate`),
      round,
      digits = 2
    )
  
}

# Student level
statistics_student <- d_transcript %>%
  group_by(`Student ID`) %>%
  provide_statistics()

# Course level
statistics_course <- d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  group_by(`Course ID`) %>%
  provide_statistics()

# Cluster level
statistics_cluster <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  provide_statistics()

# Concentration evel
statistics_concentration <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  gather(X, Concentration, Concentration, `Concentration (additional)`, na.rm = TRUE) %>%
  group_by(Concentration) %>%
  provide_statistics()

# Year level
statistics_year <- d_transcript %>%
  group_by(Year_numerical) %>%
  provide_statistics()

# Level level
statistics_level <- d_transcript %>%
  # TODO: filter for student who completed their studies
  inner_join(d_course, by = c("Course ID")) %>%
  filter(!is.na(Level)) %>%
  group_by(Level) %>%
  provide_statistics()

#
# output
save(statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level, file = "Output/Transcript Statistics.RDATA")

rm(provide_statistics, statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level)
```

# Association Rules and Sequence Rules
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

## Data Prep: Creating Transactions and Sequences
First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 

### take, fail, low grade
```{r AR data prep 1}

#
# Threshold: pass grade, high grade
pass_grade <- 5.5
high_grade <- 6.5


#
# Transactions
d_transactions <- d_course %>%
  
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  select(- Period) %>%
  inner_join(
    d_transcript,
    by = "Course ID"
    ) %>%
  
  # Identifying sequenceID
  rename(sequenceID = `Student ID`) %>%
  
  # Identifying evenID
  mutate(
    Period  = substr(Period, 1, 1),
    eventID = as.numeric(paste(Year_numerical, Period, sep = ""))
    ) %>%

  # Identifying itemID
  mutate(
    PF = case_when( Grade <  pass_grade ~ "fail",
                    Grade >= pass_grade ~ "pass"),
    HL = case_when( Grade <  high_grade ~ "low",
                    Grade >= high_grade ~ "high"),
    
    item    = `Course ID`,
    item_PF = paste(`Course ID`, PF, sep = "_"),
    item_HL = paste(`Course ID`, HL, sep = "_")
    )

```

### take / not take, pass / fail / not take
```{r transactions not taken}
d_transactions_not_taken <- expand.grid(
  
  # Expand along students (sequenceID) and courses (itemID)
  sequenceID = unique(d_transactions$sequenceID),
  item       = unique(d_transactions$item),
  stringsAsFactors = FALSE
  ) %>%
  
  # Join with d_transactions
  left_join(
    d_transactions, 
    by = c("sequenceID", "item")
    ) %>%
  
  # Create 
  mutate(
    
    taken = case_when(
      is.na(Grade) ~ "not taken",
      TRUE         ~ "taken"
      ),
    
    taken_grade = case_when(
      is.na(Grade) ~ "not taken",
      Grade < 5.5  ~ "fail",
      TRUE         ~ "pass"
      ),
    
    item_taken       = paste(item, taken      , sep = "_"),
    item_taken_grade = paste(item, taken_grade, sep = "_")
    
    )

```

### Grade
```{r transactions expanded}

d_transactions_grade <- d_transactions %>%
  
  # Spread along rounded grades
  mutate(
    Round_grade = round(Grade, 0),
    Values = TRUE
    ) %>%
  spread(
    key   = Round_grade, 
    value = Values
    ) %>%
  
  # fill grades inferior to obtained grade with TRUE
  mutate(
    `1`  = `0`|`1`,
    `2`  = `1`|`2`,
    `3`  = `2`|`3`,
    `4`  = `3`|`4`,
    `5`  = `4`|`5`,
    `6`  = `5`|`6`,
    `7`  = `6`|`7`,
    `8`  = `7`|`8`,
    `9`  = `8`|`9`,
    `10` = `9`|`10`
    ) %>%
  
  # Gather along rounded grades
  gather(
    key = Grade_round,
    value = "Gr_bool",
    `0`, `1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`, `10`, 
    na.rm = TRUE, 
    convert = TRUE
    ) %>%
  select(
    -Gr_bool
    )%>%
  
  # Arrange per sequenceID and item for making sequences
  arrange(
    sequenceID,
    item
    ) %>%

  # Identifying item
  mutate(
    item_Grade = paste(item, Grade_round, sep = "_")
    )
```

### Preceding courses
```{r}
d_cumulative_transcript <-  d_transactions        %>%
  select(sequenceID, item, eventID)  %>%
  arrange(sequenceID, eventID, item) %>%
  group_by(sequenceID, eventID)      %>%
  summarize_all(paste, collapse=" ") %>%
  arrange(sequenceID, eventID)       %>%
  mutate(
    course_all  = paste(unique(d_transactions$item), collapse = " "),
    course_all  = str_split(course_all, pattern = " "),
    
    course_now  = str_split(item, pattern = " "),

    course_past     = Reduce(paste, item, accumulate=T),
    course_past     = lag(course_past),
    course_past     = str_split(course_past, pattern = " ")) %>%
  ungroup() %>%
    mutate(
    course_past_now = purrr::map2(.x = course_past,
                                      .y = course_now,
                                      .f = union),
    course_not = purrr::map2( .x = course_all,
                              .y = course_past_now,
                              .f = setdiff)
    ) %>%
  mutate(item = str_split(item, pattern = " "))
```

###Computing probabilities
```{r}

# Number of students
n_students <- d_transactions %>%
  select(sequenceID) %>%
  n_distinct()


# Probability of taken a course
d_support_take <- d_transactions %>%
  distinct(
    item,
    sequenceID
    ) %>%
  group_by(
    item
    ) %>%
  summarise(
    count     = n(),
    rate.take = count / n_students,
    rate.not  = 1 - rate.take
    )

# Probability of failing, having a low grade of a grade less than x in a course
d_support <- d_transactions %>%
  mutate(
    grade_round = round(Grade)
  ) %>%
  group_by(
    item
    ) %>%
  summarise(
    rate.fail = mean(PF == "fail"),
    rate.low  = mean(HL == "low"),
    rate.0    = mean(grade_round <= 0),
    rate.1    = mean(grade_round <= 1),
    rate.2    = mean(grade_round <= 2),
    rate.3    = mean(grade_round <= 3),
    rate.4    = mean(grade_round <= 4),
    rate.5    = mean(grade_round <= 5),
    rate.6    = mean(grade_round <= 6),
    rate.7    = mean(grade_round <= 7),
    rate.8    = mean(grade_round <= 8),
    rate.9    = mean(grade_round <= 9),
    rate.10   = mean(grade_round <= 10)
    )


```


### Transactions for Apriori
```{r making transactions}

#
# For convenience
make_transaction <- function(data = d_transactions, item = item){
  
  data %>%
    group_by(
      sequenceID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
}


#
# Making transactions
transactions                  <- make_transaction(item = item   )
transactions_PF               <- make_transaction(item = item_PF)
transactions_HL               <- make_transaction(item = item_HL)
transactions_Taken            <- make_transaction(data = d_transactions_not_taken, item = item_taken)
transactions_expanded_Grade   <- make_transaction(data = d_transactions_expanded, item = item_Grade)

#
# Checking transactions
#inspect(head(transactions_Grade, 10))

rm(make_transaction)
```

### Sequences for CSPADE
```{r making sequences}

#
# For convenience
make_sequence <- function(data = d_transactions, item = item){
  
  sequences <- data %>%
    filter(!is.na(eventID))%>% #this is here casue we have courses that were never taken.
    group_by(
      sequenceID,
      eventID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    arrange(
      sequenceID,
      eventID
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
  sequences@itemsetInfo <- select(
    data,
    sequenceID,
    eventID
    ) %>%
    as.data.frame()
  
  return(sequences)
  
}


#
# Making sequences
sequences          <- make_sequence(item = item   )
sequences_PF       <- make_sequence(item = item_PF)
sequences_HL       <- make_sequence(item = item_HL)

sequences_expanded_Grade  <- make_sequence(data = d_transactions_expanded, item = item_Grade)

#
# Checking sequences
# inspect(head(sequences, 10))


rm(make_sequence)
```


## Mining Rules
### Apriori Algorithm

The function `my_apriori()` applies the apriori algorithm on a set of transactions with the parameters that we have chosen.
```{r my_apriori function}

my_apriori <- function(data, appearance){
  
  data %>%
    apriori(
      parameter = list(
        
        # Chosen parameters
        supp = 0,    # min support
        smax = 1,    # max support
        conf = 0,    # min confidence
        minlen = 2,  # min length of 
        maxlen = 2,  # max length of rule
        ext = TRUE,
        arem = "diff",
        aval = TRUE,
        minval = 0
        ),
      
      # impose restriction on lhs and rhs
      appearance = appearance,
      
      control = list(
        verbose = FALSE
      )
      
      )
  
}
```

The function `clean_AR()` transforms the rules generated by the function `my_apriori()` into a readable dataframe
```{r function clean AR}

clean_AR <- function(AR){
  
  AR %>%
    as("data.frame") %>%
    
    # Exclude rules with support of `0`
    filter(
      count >= 1
    ) %>%
    
    # Clean variable `rules`
    mutate(
      rules = str_remove_all(rules, pattern = "[{]"),
      rules = str_remove_all(rules, pattern = "[}]")
    ) %>%
    separate(rules, into = c("lhs", "rhs"), sep = " => ") %>%
    
    # Editing output
    mutate_at(
      c("support", "lhs.support", "confidence", "lift"),
      funs(round(., 5))
    ) %>%
    select(
      lhs, rhs,
      support, count, lhs.support,
      confidence,
      lift
    ) %>%
    arrange(
      desc(count)
    )
  
}
```

The function `compute_support()` computes the support of the lhs and rhs of the rules.

```{r}
compute_support <- function(AR, data_support, type_rule){
  
  type_rule_enquo <- enquo(type_rule)
  
  AR %>%
    
    # include lhs.support
    mutate(
      lhs_course = str_remove_all(lhs, pattern = "_fail|_low|_not taken")
      ) %>%
    left_join(
      select(data_support, item, !!enquo(type_rule)),
      by = c("lhs_course" = "item")
      ) %>%
    mutate(
      lhs.support = round(!!type_rule_enquo, 5)
      )  %>%
    select(
      - !!type_rule_enquo
      ) %>%
    
    # include rhs.support
    mutate(
      rhs_course = str_remove_all(rhs, pattern = "_fail|_low|_not taken")
      ) %>%
    left_join(
      select(data_support, item, !!type_rule_enquo),
      by = c("rhs_course" = "item")
      ) %>%
    mutate(
      rhs.support = round(!!type_rule_enquo, 5)
      ) %>%
    select(
      - !!type_rule_enquo
      ) %>%
    
    # order variable
    select(
      lhs_course, lhs, rhs, support, count, confidence, rhs.support, lift, lhs.support
    )
  
}
```

We encapsulate the three functions `my_apriori()`, `clean_AR()` and `compute_support` into the function `make_AR()`.

```{r}
make_AR <- function(data, appearance = NULL,
                    data_support, type_rule){
  
  data %>%
    my_apriori(appearance = appearance) %>%
    clean_AR %>%
    compute_support(data_support = data_support, type_rule = !!enquo(type_rule))
  
}
```


```{r AR apriori, cache = TRUE}

#
# AR taken
AR_taken <- transactions %>%
  make_AR(
    data_support = d_support_take, 
    type_rule = rate.take
    )


#
# AR fail
AR_PF <- transactions_PF %>%
  make_AR(
    appearance = list(
      
      # impose lhs and rhs to be fail
      both = d_transactions %>%
        filter(PF == "fail") %>%
        distinct(`item_PF`) %>%
        pull
    ),
    
    data_support = d_support,
    type_rule = rate.fail
  )


#
# AR low
AR_PF <- transactions_HL %>%
  make_AR(
    appearance = list(
      
      # impose lhs and rhs to be low
      both = d_transactions %>%
        filter(HL == "low") %>%
        distinct(`item_HL`) %>%
        pull
    ),
    
    data_support = d_support,
    type_rule = rate.low
  )


#
# AR not taken
AR_not_taken <- transactions_Taken %>%
  make_AR(
    appearance = list(
      
      # impose lhs and rhs to be not taken
      both = d_transactions_not_taken %>%
        filter(taken == "not taken") %>%
        distinct(`item_taken`) %>%
        pull
    ),
    
    data_support = d_support_take,
    type_rule = rate.not
  )

# TODO

# AR_expanded_Grade <- my_apriori(transactions_expanded_Grade)

```
We save and remove objects:
```{r saving and rm}
#
# Save association rules
save(AR_taken, AR_PF, AR_HL, AR_not_taken, AR_Grade, 
     AR_expanded_Grade,
     file = "App/AR.RDATA")

# Remove objects
rm(clean_AR, my_apriori,
   transactions, transactions_PF, transactions_HL, transactions_Grade,
   course_id_fail, course_id_low,
   AR_taken, AR_PF, AR_HL, AR_not_taken, AR_Grade,  
   AR_expanded_Grade)
```

### CSPADE Algorithm
##TODO: fine tune parameters of cspade and ruleInduction, not taken --> fail

```{r function clean SR}
# Transform rules from ruleInduction into a readable data frame
n_students <- length(unique(d_transactions$sequenceID))

clean_SR<- function(rules){
  
  rules %>%
    as("data.frame") %>%
    mutate(
      rule = str_remove_all(rule, pattern = "[<]"),
      rule = str_remove_all(rule, pattern = "[{]"),
      rule = str_remove_all(rule, pattern = "[}]"),
      rule = str_remove_all(rule, pattern = "[>]")
      ) %>%
    separate(rule, into = c("lhs", "rhs"), sep="=")  %>%
    select(
      lhs, rhs,
      support,
      confidence,
      lift
    ) %>%
    separate(lhs, into=c("LHS Course ID", "LHS Quality"), sep="_", remove= F) %>%
    separate(rhs, into=c("RHS Course ID", "RHS Quality"), sep="_", remove= F) %>%
    # Compute statistic
    group_by(lhs,`RHS Course ID`) %>%
    mutate(
      lhs.rhsCourse.support = sum(support),
      confidence = case_when(is.na(`RHS Quality`) ~ confidence,
                             T ~ (support / lhs.rhsCourse.support)),
      count = support * n_students,
      lhs.rhsCourse.count = lhs.rhsCourse.support * n_students,
      lift = 1 #dummy variable
      # TODO: compute rhs.support
      # lift = confidence.corr / rhs.support
     ) %>%
    ungroup() %>%
    mutate_at(
      c("support",
        "confidence", "lift"),
      funs(round(., 5))
    ) %>%
    filter(!`LHS Course ID`==`RHS Course ID`) %>%
    select(-`LHS Course ID`, -`LHS Quality`, -`RHS Course ID`, -`RHS Quality`)
    
}

```
We apply cspade followed by rule induction
```{r my_SR function}
# Run the cspade algorithm with desired parameters. Then run the ruleInduction algorithm with desired parameters.
my_SR<- function(data){
 data %>%
  cspade(
    parameter = list(
      support = 0, # min support of a sequence
      maxsize = 1, # max number of items of an element of a sequence
      maxlen  = 2, # max number of element of a sequence
      mingap  = 1, # min time difference between consecutive element of a sequence
      maxgap  = 1e4 # max time difference between consecutive element of a sequence
      #maxwin  = 1e4  # max time difference between any two elements of a sequence # WARNING: 'maxwin' disabled
      ),
    control = list(
      verbose = FALSE
      )
    ) %>%
    ruleInduction(
      confidence = 0, #we need to play with the parameters.
      control    = list(verbose = FALSE)
      ) %>%
    clean_SR
}
```

We generate sequence rules:
```{r SR taken, cache = TRUE}

#Generating sequence rules:
SR_taken   <- my_SR(data = sequences   )
```

```{r SR PF, cache = TRUE}

##pass/fail filter
SR_PF      <- my_SR(data = sequences_PF) %>%
  filter(
    str_detect(rhs, "fail"),
    str_detect(lhs, "fail")
    )
```

```{r SR HL, cache = TRUE}

##high/low filter
SR_HL      <- my_SR(data = sequences_HL) %>% 
  filter(
    str_detect(rhs, "low"),
    str_detect(lhs, "low")
    )
```

```{r SR grade, cache = TRUE}

SR_Grade   <- my_SR(data = sequences_Grade )
```

```{r SR grade exp, cache = TRUE}

#Grades filter 
SR_expanded_Grade   <- my_SR(data = sequences_expanded_Grade )
```

We save and remove objects:
```{r saving and rm SR}
#Saving sequence rules
save(
    SR_taken, SR_PF, SR_HL, SR_Grade,
    SR_expanded_Grade,
    file = "App/SR.RDATA")

#clean unnecessary objects
rm(clean_SR, my_SR, 
   sequences, sequences_PF, sequences_HL, sequences_Grade,
   sequences_expanded_Grade,
   n_students,
   SR_taken, SR_PF, SR_HL, SR_Grade,

   SR_expanded_Grade)
```


