---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---

```{r library, message = FALSE, warning = FALSE}
library(tidyverse)
library(arulesSequences)
```

# Setup
We load the environment `data_pillar1` which we saved at the end of the data preparation. It contains the data sets `d_course` and `d_transcript`.
```{r loading data}
load("Output/data_pillar_1.RDATA")
```

We create a function, which, when given the code of a course, returns its title.
```{r function find_course}
find_course <- function(code){ 
  
  dataset <- d_transcript %>%
    filter(`Course ID`== code)
  
  title <- dataset$`Course Title`[1]
  
  return(title)
  
}

# Example
find_course("HUM1005")
```

# Data Exploration

We compute summary statistics (minimum, maximum, mean, median, standard deviation, failure rate, number of failure and count) at different levels (student, course, cluster, concentration, year and course level). We save the results in the environment `Transcript Statistics`.

```{r statistic summary}

# For convenience
provide_statistics <- function(data){
  
  data %>%
    summarise(
      Min       = min(Grade),
      Max       = max(Grade), 
      Mean      = mean(Grade), 
      Median    = median(Grade), 
      SD        = sd(Grade),
      `Failure Rate` = mean(Fail),
      `Failure Count`    = sum(Fail),
      Count     = n()
      ) %>%
    mutate_at(
      vars(Mean, SD, `Failure Rate`),
      round,
      digits = 2
    )
  
}

# Student level
statistics_student <- d_transcript %>%
  group_by(`Student ID`) %>%
  provide_statistics()

# Course level
statistics_course <- d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  group_by(`Course ID`) %>%
  provide_statistics()

# Cluster level
statistics_cluster <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  provide_statistics()

# Concentration evel
statistics_concentration <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  gather(X, Concentration, Concentration, `Concentration (additional)`, na.rm = TRUE) %>%
  group_by(Concentration) %>%
  provide_statistics()

# Year level
statistics_year <- d_transcript %>%
  group_by(Year_numerical) %>%
  provide_statistics()

# Level level
statistics_level <- d_transcript %>%
  # TODO: filter for student who completed their studies
  inner_join(d_course, by = c("Course ID")) %>%
  filter(!is.na(Level)) %>%
  group_by(Level) %>%
  provide_statistics()

#
# output
save(statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level, file = "Output/Transcript Statistics.RDATA")

rm(provide_statistics, statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level)
```

# Association Rules and Sequence Rules
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

## Data Prep: Creating Transactions and Sequences
First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 

### take, fail, low grade
```{r AR data prep 1}

#
# Threshold: pass grade, high grade
pass_grade <- 5.5
high_grade <- 6.5


#
# Transactions
d_transactions <- d_course %>%
  
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  select(- Period) %>%
  inner_join(
    d_transcript,
    by = "Course ID"
    ) %>%
  
  # Identifying sequenceID
  rename(sequenceID = `Student ID`) %>%
  
  # Identifying evenID
  mutate(
    Period  = substr(Period, 1, 1),
    eventID = as.numeric(paste(Year_numerical, Period, sep = ""))
    ) %>%

  # Identifying itemID
  mutate(
    PF = case_when( Grade <  pass_grade ~ "fail",
                    Grade >= pass_grade ~ "pass"),
    HL = case_when( Grade <  high_grade ~ "low",
                    Grade >= high_grade ~ "high"),
    
    item    = `Course ID`,
    item_PF = paste(`Course ID`, PF, sep = "_"),
    item_HL = paste(`Course ID`, HL, sep = "_")
    )

```

### take / not take, pass / fail / not take
```{r transactions not taken}
d_transactions_TPF <- expand.grid(
  
  # Expand along students (sequenceID) and courses (itemID)
  sequenceID = unique(d_transactions$sequenceID),
  item       = unique(d_transactions$item),
  stringsAsFactors = FALSE
  ) %>%
  
  # Join with d_transactions
  left_join(
    d_transactions, 
    by = c("sequenceID", "item")
    ) %>%
  
  # Create 
  mutate(
    
    TPF = case_when(
      is.na(Grade)       ~ "not taken",
      Grade < pass_grade ~ "fail",
      TRUE               ~ "pass"
      ),

    item_TPF = paste(item, TPF, sep = "_")
    
    )

rm(high_grade, pass_grade)
```

### Grade
```{r transactions grade}

d_transactions_G <- d_transactions %>%
  
  # Spread along rounded grades
  mutate(
    grade_ceil = ceiling(Grade),
    Values     = TRUE
    ) %>%
  spread(
    key   = grade_ceil, 
    value = Values
    ) %>%
  
  # fill grades inferior to obtained grade with TRUE
  mutate(
    `1`  = `0`|`1`,
    `2`  = `1`|`2`,
    `3`  = `2`|`3`,
    `4`  = `3`|`4`,
    `5`  = `4`|`5`,
    `6`  = `5`|`6`,
    `7`  = `6`|`7`,
    `8`  = `7`|`8`,
    `9`  = `8`|`9`,
    `10` = `9`|`10`
    ) %>%
  
  # Gather along rounded grades
  gather(
    key   = grade_ceil,
    value = X,
    `0` :`10`, 
    na.rm   = TRUE, 
    convert = TRUE
    ) %>%
  select(
    -X
    ) %>%

  # Identifying item
  mutate(
    item_G = paste(item, grade_ceil, sep = "_")
    )
```
### Additional General Dataframes for courses
#### Transcript with Preceding courses per student
```{r}
d_cumulative_transcript <-  d_transactions        %>%
  select(sequenceID, item, eventID)  %>%
  arrange(sequenceID, eventID, item) %>%
  group_by(sequenceID, eventID)      %>%
  summarize_all(paste, collapse=" ") %>%
  arrange(sequenceID, eventID)       %>%
  mutate(
    course_all  = paste(unique(d_transactions$item), collapse = " "),
    course_all  = str_split(course_all, pattern = " "),
    
    course_now  = str_split(item, pattern = " "),

    course_past     = Reduce(paste, item, accumulate=T),
    course_past     = lag(course_past),
    course_past     = str_split(course_past, pattern = " ")) %>%
  ungroup() %>%
    mutate(
    course_past_now = purrr::map2(.x = course_past,
                                      .y = course_now,
                                      .f = union),
    course_not = purrr::map2( .x = course_all,
                              .y = course_past_now,
                              .f = setdiff)
    ) %>%
  mutate(item = str_split(item, pattern = " "))
```

#### Courses that were taken before each course
```{r}
d_past_student_course <- d_cumulative_transcript %>%
  select(sequenceID, eventID, item, course_past) %>%
  unnest(item, .drop=F)

d_past_course <- expand.grid(
  
  # Expand along students (sequenceID) and courses (itemID)
  sequenceID = unique(d_transactions$sequenceID),
  item       = unique(d_transactions$item),
  stringsAsFactors = FALSE
  ) %>%
  full_join(d_past_student_course, by= c("sequenceID", "item")) %>%  #ATTENTION: Duplicates resits. Adds values per retake of course (eg.10177849 SCI3048)
  arrange(sequenceID) %>%
  ungroup() %>%
  drop_na(eventID) %>%
  unnest(course_past) %>%
  distinct() %>%
  group_by(item) %>%
  summarise(full_past = paste( course_past, collapse = " ")) %>% #difference, between this and above?
  mutate(full_past    = str_split(full_past, pattern = " "),
         full_past    = map(.f= unique, .x=full_past),
         total_past_courses = map(.f=length, .x=full_past))
```

### Support
```{r}

#
# Number of students
n_students <- d_transactions %>%
  select(sequenceID) %>%
  n_distinct


#
# Probability of taking a course
d_support_TNT <- d_transactions %>%
  distinct(
    item,
    sequenceID
    ) %>%
  count(
    item
    ) %>%
  mutate(
    rate.take = n / n_students,
    rate.not  = 1 - rate.take
    )


#
# Probability of failing, having a low grade
d_support_PF_HL <- d_transactions %>%
  group_by(
    item
    ) %>%
  summarise(
    rate.fail = mean(PF == "fail"),
    rate.low  = mean(HL == "low")
    )


#
# Probability oh having a grade equal or lower than x
d_support_G <- d_transactions %>%
  group_by(
    item
    ) %>%
  summarise(
    `0`    = mean(Grade <= 0),
    `1`    = mean(Grade <= 1),
    `2`    = mean(Grade <= 2),
    `3`    = mean(Grade <= 3),
    `4`    = mean(Grade <= 4),
    `5`    = mean(Grade <= 5),
    `6`    = mean(Grade <= 6),
    `7`    = mean(Grade <= 7),
    `8`    = mean(Grade <= 8),
    `9`    = mean(Grade <= 9),
    `10`   = mean(Grade <= 10)
    ) %>%
  gather(
    key   = grade_ceil,
    value = support.grade,
    `0` : `10`,
    convert = TRUE
    )

```


### Transactions for Apriori
```{r making transactions, message = FALSE}

make_transaction <- function(data = d_transactions, item = item){
  
  data %>%
    group_by(
      sequenceID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
}


#
# Making transactions
transactions     <- make_transaction(item = item   )
transactions_PF  <- make_transaction(item = item_PF)
transactions_HL  <- make_transaction(item = item_HL)
transactions_TPF <- make_transaction(data = d_transactions_TPF, item = item_TPF)
transactions_G   <- make_transaction(data = d_transactions_G  , item = item_G  )


#
# Checking transactions
# inspect(head(transactions, 10))

rm(make_transaction)
```

### Sequences for CSPADE
```{r making sequences}

make_sequence <- function(data = d_transactions, item = item){
  
  sequences <- data %>%
    filter(
      !is.na(eventID) #this is here casue we have courses that were never taken.
      ) %>% 
    group_by(
      sequenceID,
      eventID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    arrange(
      sequenceID,
      eventID
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
  # indicating sequence ID and event ID for each transaction
  sequences@itemsetInfo <- select(
    data,
    sequenceID,
    eventID
    ) %>%
    as.data.frame
  
  return(sequences)
  
}


#
# Making sequences
sequences     <- make_sequence(item = item   )
sequences_PF  <- make_sequence(item = item_PF)
sequences_HL  <- make_sequence(item = item_HL)
#sequences_TPF <- make_sequence(data = d_transactions_TPF, item = item_TPF) # compute ad hoc
sequences_G   <- make_sequence(data = d_transactions_G    , item = item_G  )


#
# Checking sequences
# inspect(head(sequences_G, 10))


rm(make_sequence,
   d_transactions, d_transactions_TPF, d_transactions_G)
```


## Mining Rules
### Apriori Algorithm

The function `my_apriori()` applies the apriori algorithm on a set of transactions with the parameters that we have chosen.
```{r my_apriori}
my_apriori <- function(data){
  
  data %>%
    apriori(
      
      # include all AR possible
      parameter = list(
        supp   = 0, # min support
        smax   = 1, # max support
        conf   = 0, # min confidence
        minlen = 2, # min length of rule
        maxlen = 2  # max length of rule
        ),
      
      # no printing during execution
      control = list(
        verbose = FALSE
        )
      
      )
  
}
```

The function `clean_AR()` transforms the rules generated by the function `my_apriori()` into a readable dataframe
```{r clean_AR}
clean_AR <- function(AR){
  
  AR %>%
    as("data.frame") %>%
    
    # Exclude rules with no support
    filter(
      count >= 1
      ) %>%
    
    # Clean variable `rules`
    mutate(
      rules = str_remove_all(rules, pattern = "[{]"),
      rules = str_remove_all(rules, pattern = "[}]")
      ) %>%
    separate(
      rules,
      into = c("lhs", "rhs"), 
      sep  = " => "
      )
  
}
```

The function `compute_support()` computes the support of the lhs and rhs of the rules.

```{r compute_support}
compute_metrics <- function(AR, data_support, type_rule){

  AR %>%
    
    # rhs.support
    mutate(
      rhs_course = str_remove_all(rhs, pattern = "_fail|_pass|_low|_not taken")
      ) %>%
    left_join(
      select(data_support, item, !!enquo(type_rule)),
      by = c("rhs_course" = "item")
      ) %>%
    mutate(
      rhs.support = !!enquo(type_rule)
      )

}
```

```{r editing_AR}
editing_AR <- function(AR){
  
  AR %>%
    mutate_at(
      c("support", 
        "confidence", "rhs.support",
        "lift"
        ),
      funs(round(., 5))
    ) %>%
    select(
      lhs, rhs,
      support, count,
      confidence, rhs.support,
      lift
    ) %>%
    arrange(
      desc(count)
    )
  
}
```


We encapsulate the four functions `my_apriori()`, `clean_AR()`, `compute_support()` and `editing_AR()` into the function `make_AR()`.

```{r make_AR}
make_AR <- function(data, data_support, type_rule){
  
  data %>%
    my_apriori %>%
    clean_AR %>%
    compute_metrics(
      data_support = data_support,
      type_rule = !!enquo(type_rule)
      ) %>%
    editing_AR
  
}
```

```{r AR take}
AR_taken <- transactions %>%
  make_AR(
    data_support = d_support_TNT, 
    type_rule = rate.take
    )

rm(transactions, d_support_TNT)
```

```{r AR fail}
AR_PF <- transactions_PF %>%
  
  make_AR(
    data_support = d_support_PF_HL,
    type_rule = rate.fail
    ) %>%
  
  # Confidence and Lift by hand
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  group_by(
    lhs, rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = sum(support),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / rhs.support
    ) %>%
  ungroup %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "fail"),
    str_detect(rhs, "fail"),
    count >= 5
    ) %>%
  select(
    - c(rhs_outcome, rhs_course)
  )

rm(transactions_PF)
```

```{r AR low}
AR_HL <- transactions_HL %>%
  
  make_AR(
    data_support = d_support_PF_HL,
    type_rule = rate.low
    ) %>%
  
  # Confidence and Lift by hand
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  group_by(
    lhs, rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = sum(support),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / rhs.support
    ) %>%
  ungroup %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "low"),
    str_detect(rhs, "low"),
    count >= 5
    ) %>%
  select(
    - c(rhs_outcome, rhs_course)
  )

rm(transactions_HL)
```

```{r AR TPF}
AR_TPF <- transactions_TPF %>%
  
  make_AR(
    data_support = d_support_PF_HL, 
    type_rule = rate.fail
  ) %>%
  
  # Confidence and Lift
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  mutate(
    rhs.take = str_detect(rhs, "fail|pass")
    ) %>%
  group_by(
    lhs, rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = sum(support * rhs.take),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / rhs.support
    ) %>%
  ungroup %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "not taken"),
    str_detect(rhs, "fail"),
    count >= 5
    ) %>%
  select(
    - c(rhs_outcome, rhs_course, rhs.take)
  )

rm(transactions_TPF, d_support_PF_HL)
```

```{r AR grade}
AR_G <- transactions_G %>%
  
  my_apriori %>%
  clean_AR %>%

  # separate rhs and lhs
  separate(
    rhs,
    into = c("rhs_course", "rhs_grade"),
    sep = "_",
    remove = FALSE,
    convert = TRUE
    ) %>%
  separate(
    lhs,
    into = c("lhs_course", "lhs_grade"),
    sep = "_",
    remove = FALSE,
    convert = TRUE
    ) %>%
  
  # rhs.support
  left_join(
    d_support_G,
    by = c("rhs_course" = "item", "rhs_grade" = "grade_ceil")
    ) %>%
  mutate(
    rhs.support = support.grade
    ) %>%
  
  # Confidence and lift
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  group_by(
    lhs, 
    rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = max(support),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / rhs.support
    ) %>%
  ungroup %>%
  
  
  # reduce number of rules
  filter(
    lhs_course != rhs_course, # exclude rules with same course on lhs and rhs
    count >= 5,               
    rhs_grade == 6            
    ) %>%
  
  # for each combination of courses (lhs and rhs), keep AR with highest lift (most informative AR)
  group_by(
    lhs_course, 
    rhs_course
    ) %>%
  top_n(
    1,
    lift
    ) %>%
  ungroup %>%

  editing_AR


rm(transactions_G, d_support_G)
```

We save and remove objects:
```{r saving and rm}
#
# Save association rules
save(AR_taken, AR_PF, AR_HL, AR_TPF, AR_G,
     file = "App/AR.RDATA")

# Remove objects
rm(my_apriori, clean_AR, compute_metrics, editing_AR, make_AR,
   AR_taken, AR_PF, AR_HL, AR_TPF, AR_G,
   n_students)
```

### CSPADE Algorithm
##TODO: fine tune parameters of cspade and ruleInduction, not taken --> fail

```{r function clean SR}
# Transform rules from ruleInduction into a readable data frame

clean_SR<- function(rules){
  
  rules %>%
    as("data.frame") %>%
    mutate(
      rule = str_remove_all(rule, pattern = "[<]"),
      rule = str_remove_all(rule, pattern = "[{]"),
      rule = str_remove_all(rule, pattern = "[}]"),
      rule = str_remove_all(rule, pattern = "[>]")
      ) %>%
    separate(rule, into = c("lhs", "rhs"), sep="=")  %>%
    select(
      lhs, rhs,
      support,
      confidence,
      lift
    ) %>%
    separate(lhs, into=c("LHS Course ID", "LHS Quality"), sep="_", remove= F) %>%
    separate(rhs, into=c("RHS Course ID", "RHS Quality"), sep="_", remove= F) %>%
    # Compute statistic
    group_by(lhs,`RHS Course ID`) %>%
    mutate(
      lhs.rhsCourse.support = sum(support),
      confidence = case_when(is.na(`RHS Quality`) ~ confidence,
                             T ~ (support / lhs.rhsCourse.support)),
      count = support * n_students,
      lhs.rhsCourse.count = lhs.rhsCourse.support * n_students,
      lift = 1 #dummy variable
      # TODO: compute rhs.support
      # lift = confidence.corr / rhs.support
     ) %>%
    ungroup() %>%
    mutate_at(
      c("support",
        "confidence", "lift"),
      funs(round(., 5))
    ) %>%
    filter(!`LHS Course ID`==`RHS Course ID`) %>%
    select(-`LHS Course ID`, -`LHS Quality`, -`RHS Course ID`, -`RHS Quality`)
    
}

```
We apply cspade followed by rule induction
```{r my_SR function}
# Run the cspade algorithm with desired parameters. Then run the ruleInduction algorithm with desired parameters.
my_SR<- function(data){
 data %>%
  cspade(
    parameter = list(
      support = 0, # min support of a sequence
      maxsize = 1, # max number of items of an element of a sequence
      maxlen  = 2, # max number of element of a sequence
      mingap  = 1, # min time difference between consecutive element of a sequence
      maxgap  = 1e4 # max time difference between consecutive element of a sequence
      #maxwin  = 1e4  # max time difference between any two elements of a sequence # WARNING: 'maxwin' disabled
      ),
    control = list(
      verbose = FALSE
      )
    ) %>%
    ruleInduction(
      confidence = 0, #we need to play with the parameters.
      control    = list(verbose = FALSE)
      ) %>%
    clean_SR
}
```

We generate sequence rules:
```{r SR taken, cache = TRUE}

#Generating sequence rules:
SR_taken   <- my_SR(data = sequences   )
```

```{r SR PF, cache = TRUE}

##pass/fail filter
SR_PF      <- my_SR(data = sequences_PF) %>%
  filter(
    str_detect(rhs, "fail"),
    str_detect(lhs, "fail")
    )
```

```{r SR HL, cache = TRUE}

##high/low filter
SR_HL      <- my_SR(data = sequences_HL) %>% 
  filter(
    str_detect(rhs, "low"),
    str_detect(lhs, "low")
    )
```

```{r SR grade, cache = TRUE}

SR_Grade   <- my_SR(data = sequences_Grade )
```

```{r SR grade exp, cache = TRUE}

#Grades filter 
SR_expanded_Grade   <- my_SR(data = sequences_expanded_Grade )
```

We save and remove objects:
```{r saving and rm SR}
#Saving sequence rules
save(
    SR_taken, SR_PF, SR_HL, SR_Grade,
    SR_expanded_Grade,
    file = "App/SR.RDATA")

#clean unnecessary objects
rm(clean_SR, my_SR, 
   sequences, sequences_PF, sequences_HL, sequences_Grade,
   sequences_expanded_Grade,
   n_students,
   SR_taken, SR_PF, SR_HL, SR_Grade,

   SR_expanded_Grade)
```


