---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---

```{r library, message = F}
library(arulesSequences)

library(tidyverse)
```

# Setup
load previous data and create function to find courses
```{r loading data}
load("Output/data_pillar_1.RDATA")
```

Function that returns title of a course given its code.
```{r function find_course}
find_course <- function(code){ 
  
  dataset <- d_transcript %>%
    filter(`Course ID`== code)
  
  title <- dataset$`Course Title`[1]
  
  return(title)
  
}

# Example
find_course("HUM1005")
```

# Data Exploration

```{r statistic summary}

# For convenience
provide_statistics <- function(data){
  data %>%
    summarise(
      Min       = min(Grade),
      Max       = max(Grade), 
      Mean      = round(mean(Grade), 2), 
      Median    = median(Grade), 
      SD        = round(sd(Grade), 2),
      Fail_rate = round(mean(Fail), 2),
      Fail_n    = sum(Fail),
      n         = n()
      )
}

# Student level
statistics_student <- d_transcript %>%
  group_by(`Student ID`) %>%
  provide_statistics()

# Course level
statistics_course <- d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  group_by(`Course ID`) %>%
  provide_statistics()

# Cluster level
statistics_cluster <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  provide_statistics()

# Concentration evel
statistics_concentration <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  gather(X, Concentration, Concentration, `Concentration (additional)`, na.rm = TRUE) %>%
  group_by(Concentration) %>%
  provide_statistics()

# Year level
statistics_year <- d_transcript %>%
  group_by(Year_numerical) %>%
  provide_statistics()

# Level level
statistics_level <- d_transcript %>%
  # TODO: filter for student who completed their studies
  inner_join(d_course, by = c("Course ID")) %>%
  filter(!is.na(Level)) %>%
  group_by(Level) %>%
  provide_statistics()

#
# output
save(statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level, file = "Output/Transcript Statistics.RDATA")
rm(provide_statistics, statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level)
```

# Association Rules and Sequence Rules
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

## Data Prep: Creating Transactions and Sequences
First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 

```{r AR data prep 1}

#
# Threshold: pass grade, high grade
pass_grade <- 5.5
high_grade <- 6.5



#
# Transactions
d_transactions <- d_course %>%
  
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  select(- Period) %>%
  inner_join(d_transcript, by = "Course ID") %>%
  
  # Identifying sequences
  rename(sequenceID = `Student ID`) %>%
  
  # Identifying time of event (sequence)
  mutate(
    Period  = substr(Period, 1, 1),
    eventID = as.numeric(paste(Year_numerical, Period, sep = ""))
    ) %>%

  # Identifying item
  mutate(
    PF = case_when( Grade <  pass_grade ~ "fail",
                    Grade >= pass_grade ~ "pass"),
    HL = case_when( Grade <  high_grade ~ "low",
                    Grade >= high_grade ~ "high"),
    
    item_PF = paste(`Course ID`, PF, sep = "_"),
    item_HL = paste(`Course ID`, HL, sep = "_")
    ) %>%
  rename(
    item = `Course ID`
  )
```
### Adding not taken courses
```{r}
d_transactions_tmp <- expand.grid(sequenceID = unique(d_transactions$sequenceID),item = unique(d_transactions$item)) %>%
  left_join(d_transactions, by = c("sequenceID","item")) %>%
  mutate(
    Taken= case_when(
     is.na(Grade)  ~ "not taken",
      T             ~ "taken"),
    item_Taken= paste(item,Taken, sep = "_")
     )

#ARRANGE PROPERLY

d_transactions <- d_transactions_tmp

rm(d_transactions_tmp)
```



### Transactions for Apriori
```{r making transactions}

#
# For convenience
make_transaction <- function(data = d_transactions, item = item){
  
  data <- data %>%
    group_by(
      sequenceID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    ungroup
  
  transactions <- as(
    data$list_item,
    "transactions"
    )
  
  return(transactions)
  
}


#
# Making transactions
transactions       <- make_transaction(item = item   )
transactions_PF    <- make_transaction(item = item_PF)
transactions_HL    <- make_transaction(item = item_HL)
transactions_Taken <- make_transaction(item=item_Taken)

#
# Checking transactions
#inspect(head(transactions, 10))

rm(make_transaction)
```

### Sequences for CSPADE
```{r}

#
# For convenience
make_sequence <- function(data = d_transactions, item = item){
  
  data <- data %>%
    filter(!is.na(eventID))%>% #this is here casue we have courses that were never taken.
    group_by(
      sequenceID,
      eventID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    arrange(
      sequenceID,
      eventID
    ) %>%
    ungroup
  
  sequences <- as(
    data$list_item,
    "transactions"
    )
  
  sequences@itemsetInfo <- select(
    data,
    sequenceID,
    eventID
    ) %>%
    as.data.frame()
  
  return(sequences)
  
}


#
# Making sequences
sequences    <- make_sequence(item = item   )
sequences_PF <- make_sequence(item = item_PF)
sequences_HL <- make_sequence(item = item_HL)


#
# Checking sequences
#inspect(head(sequences, 10))


rm(make_sequence)
```


## Mining Rules
### Apriori Algorithm
```{r}

# Transform AR from apriori into a readable data frame
clean_AR <- function(AR){
  
  AR %>%
    as("data.frame") %>%
    filter(
      count >= 1
    ) %>%
    mutate(
      rules = str_remove_all(rules, pattern = "[{]"),
      rules = str_remove_all(rules, pattern = "[}]"),
      `rhs.support` = confidence / lift
    ) %>%
    separate(rules, into = c("lhs", "rhs"), sep="=>") %>%
    mutate_at(
      c("support", "lhs.support", "confidence", "rhs.support", "lift"),
      funs(round(., 5))
    ) %>%
    select(
      lhs, rhs,
      support, count, lhs.support,
      confidence, rhs.support,
      lift
    ) %>%
    arrange(
      desc(count)
    )
  
}
```

We apply Apriori algorithm:
```{r AR apriori}

# Run the apriori algorithm with desired parameters
my_apriori <- function(data, appearance = NULL){
  
  data %>%
    apriori(
      parameter = list(
        supp = 0,    # min support
        smax = 1,    # max support
        conf = 0,    # min confidence
        minlen = 2,  # min length of 
        maxlen = 2,  # max length of rule
        ext = TRUE,
        arem = "diff",
        aval = TRUE,
        minval = 0
        ),
      appearance = appearance,
      control = list(
        verbose = FALSE
        )
      ) %>%
    clean_AR
  
}


#
# Generating asocciation rules
AR <- my_apriori(transactions)

# creating vector of fail course
course_id_fail <- d_transactions %>%
  filter(PF == "fail") %>%
  distinct(`item_PF`)

AR_PF <- my_apriori(transactions_PF,
                    appearance = list(both = course_id_fail$`item_PF`))

# creating vector of low score course
course_id_low <- d_transactions %>%
  filter(HL == "low") %>%
  distinct(`item_HL`)

AR_HL <- my_apriori(transactions_HL,
                    appearance = list(both = course_id_low$`item_HL`))



AR_Taken <- my_apriori(transactions_Taken)


#
# Save association rules
save(AR, AR_PF, AR_HL, AR_Taken,
     file = "Output/AS.RDATA")

# TODO: MAKE APP



rm(clean_AR, my_apriori,
   transactions, transactions_PF, transactions_HL,
   course_id_fail, course_id_low,
   AR, AR_PF, AR_HL, AR_Taken)

```


### CSPADE Algorithm

```{r}

subsequences <- sequences %>%
  cspade(
    parameter = list(
      support = 0.01, # min support of a sequence
      maxsize = 1, # max number of items of an element of a sequence
      maxlen  = 2, # max number of element of a sequence
      mingap  = 1, # min time difference between consecutive element of a sequence
      maxgap  = 1e4, # max time difference between consecutive element of a sequence
      maxwin  = 1e4  # max time difference between any two elements of a sequence
      ),
    control = list(
      verbose = FALSE
      )
    )

#Inspect results CSPADE
summary(subsequences)
as(subsequences, "data.frame")
```

##rule induction:
```{r}
rules <- ruleInduction(subsequences,
                       confidence = 0, #we need to play with the parameters.
                       control    = list(verbose = TRUE))


pass_rules_df <- as(rules, "data.frame") %>%
  
  separate(sequece, into=c("LHS", "RHS"), sep = "=>", remove = T) %>% #remove = F to keep columns
  
#  filter(str_detect(RHS, string_filter)) %>% #FILTER (YES) --> uncomment, (NO)-->comment
  
  arrange(
    LHS, RHS,
    desc(confidence), desc(lift))

```



```{r}

# Transform AR from apriori into a readable data frame
clean_rules <- function(rules){
  rules %>%
    as("data.frame") %>%
    # filter(
    #   count >= 1
    # ) %>%
    mutate(`rhs.support` = confidence / lift) %>%
    separate(rule, into = c("lhs", "rhs"), sep="=>") %>%
    mutate(
      lhs = str_remove(lhs, pattern = "[<]"),
      lhs = str_remove(lhs, pattern = "[{]"),
      lhs = str_remove(lhs, pattern = "[}]"),
      lhs = str_remove(lhs, pattern = "[>]"),
      rhs = str_remove(rhs, pattern = "[<]"),
      rhs = str_remove(rhs, pattern = "[{]"),
      rhs = str_remove(rhs, pattern = "[}]"),
      rhs = str_remove(rhs, pattern = "[>]")
    ) %>%
    mutate_at(
      c("support", #"lhs.support",
        "confidence", "rhs.support", "lift"),
      funs(round(., 5))
    ) %>%
    select(
      lhs, rhs,
      support, #count, lhs.support,
      confidence, rhs.support,
      lift
    ) #%>%
    #arrange(
    #  desc(count)
    #)
  
}
```
##calculate appropriate measures:
```{r}
n_students <- length(unique(d_sequence_transactions$`Student ID`))

sequence_rules <- pass_rules_df %>%
  separate(LHS, into=c("LHS Course ID", "LHS Score"), sep="_", remove= FALSE) %>%
  separate(RHS, into=c("RHS Course ID", "RHS Score"), sep="_", remove= FALSE) %>%
  mutate(`RHS Course ID`= substr(`RHS Course ID`, start=4, stop=10))%>%
  left_join(lookup_table, by = c("RHS Course ID"="Course ID")) %>%
  
  # Compute statistics
  group_by(LHS,`RHS Course ID`) %>%
  mutate(Prob_both = sum(support), 
         Conf_corr = support/Prob_both,
         n_rule = support * n_students,
         n_both =Prob_both * n_students,
         lift_diff= Conf_corr - Prop_low,
         lift_corr=Conf_corr / Prop_low) %>%
  ungroup() %>%
  
  # Filter and edit output
  filter(str_detect(`RHS Score`, pattern = "low"),
         str_detect(`LHS Score`, pattern = "low"),
         n_rule >= 5,
         Conf_corr > 0.2) %>%
  select(LHS, RHS, support, n_rule, Prob_both, n_both, Prop_low, Conf_corr, lift_corr, lift_diff) %>%
  mutate(support = round(support,3),
         Prob_both =round(Prob_both,3),
         Prop_low =round(Prop_low,3),
         Conf_corr= round(Conf_corr,3),
         lift_diff= round(lift_diff,3),
         lift_corr= round(lift_corr,3)) %>%
  arrange(lift_diff)

```
##Save data
```{r}
save(sequence_rules,
     file = "sequence_rules.RDATA")
```

##Arrange data for plots:
```{r}
for_plots <-  manual_confidence_rules %>%
  group_by(LHS)%>%
  mutate(LHS_count=n())
```
#Visualizing rules
```{r}
#look at counts
ggplot(for_plots, aes(LHS))+
  geom_bar()

#jitter
ggplot(manual_confidence_rules)+
  geom_jitter(mapping=aes(x=Conf_corr, y=log(support), color=lift_corr))

ggplot(manual_confidence_rules)+
  geom_jitter(mapping=aes(x=log(support), y=lift_corr, color = Conf_corr))

#heatmap
ggplot(select(manual_confidence_rules,LHS,RHS, n_rule), aes(x=RHS,y=LHS, fill=n_rule))+
  geom_raster() +
  theme(
    axis.text.x = element_text(angle=90)
  )

gglot(table(select(manual_confidence_rules,LHS,RHS)))

#ggnetwork

#SEE: http://kateto.net/network-visualization 
g_net <- graph_from_data_frame(d=head(for_plots,50), directed=T) #I BELIEVE THE COLUMNS SHOULD BE REVERSED.

l <- layout_in_circle(g_net)
plot(g_net, #vertex.shape="none",
     vertex.size=.9,
     #vertex.label=NA,
     vertex.label.cex=0.5,
     edge.arrow.size=.1, 
     edge.curved=.1,
     layout=l)

```
#Visualisation
It might be a nice idea to visualise using Sankey diagrams, although I don't know if there will be too many courses for it to be meaningful. We might need to sample. See https://analyzecore.com/2014/10/31/sequence-carts-analysis-sankey/ 

#EXPERIMENTS:

##TO MOVE UP (These are ready:)
Rounds grades and add new row from original grade up to 10.
NOTE: includes mandatory courses.
```{r}
distributed_transcript <- d_transcript %>%
  filter(!is.na(Grade), Grade>=0) %>%
  mutate(Round_grade = paste("g", round(Grade,0),sep=""),
         Values = T) %>%
  spread(key = "Round_grade", value = Values) %>%
  mutate(g1  = g0|g1,
         g2  = g1|g2,
         g3  = g2|g3,
         g4  = g3|g4,
         g5  = g4|g5,
         g6  = g5|g6,
         g7  = g6|g7,
         g8  = g7|g8,
         g9  = g8|g9,
         g10 = g9|g10,
         ) %>%
  gather(key="Gr_string", value="Gr_bool", g0, g1, g2, g3, g4,g5,g6,g7,g8,g9,g10, na.rm=T) %>%
  select(-Gr_bool )%>%
  separate(Gr_string, into=c("Trash", "Grade_new"), sep="g")%>%
  select(-Trash) %>%
  mutate(Grade= as.numeric(Grade_new))%>%
  select(-Grade_new) #%>%
  #to check that the grades appear from original grade to 10.
  #arrange(`Student ID`, `Course ID`) 
```

##Not ready to move up:


