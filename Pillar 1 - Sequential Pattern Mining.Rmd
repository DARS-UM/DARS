---
title: "Pillar 1 - Sequential Pattern Mining"
author: "DARS"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: TRUE
---

```{r TODO}

```

```{r library, message = FALSE, warning = FALSE}
library(tidyverse)
library(arulesSequences)
```

# Setup
We load the environment `data_pillar1` which we saved at the end of the data preparation. It contains the data sets `d_course` and `d_transcript`.
```{r loading data}
load("Output/data_pillar_1.RDATA")
```

We create a function, which, when given the code of a course, returns its title.
```{r function find_course}
find_course <- function(code){ 
  
  dataset <- d_transcript %>%
    filter(`Course ID`== code)
  
  title <- dataset$`Course Title`[1]
  
  return(title)
  
}

# Example
find_course("HUM1005")
```

# Data Exploration

We compute summary statistics (minimum, maximum, mean, median, standard deviation, failure rate, number of failure and count) at different levels (student, course, cluster, concentration, year and course level). We save the results in the environment `Transcript Statistics`.

```{r statistic summary}

# For convenience
provide_statistics <- function(data){
  
  data %>%
    summarise(
      Min       = min(Grade),
      Max       = max(Grade), 
      Mean      = mean(Grade), 
      Median    = median(Grade), 
      SD        = sd(Grade),
      `Failure Rate` = mean(Fail),
      `Failure Count`    = sum(Fail),
      Count     = n()
      ) %>%
    mutate_at(
      vars(Mean, SD, `Failure Rate`),
      round,
      digits = 2
    )
  
}

# Student level
statistics_student <- d_transcript %>%
  group_by(`Student ID`) %>%
  provide_statistics()

# Course level
statistics_course <- d_transcript %>%
  inner_join(d_course, by = c("Course ID")) %>%
  group_by(`Course ID`) %>%
  provide_statistics()

# Cluster level
statistics_cluster <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  provide_statistics()

# Concentration evel
statistics_concentration <- d_transcript %>%
  inner_join(d_course, by = "Course ID") %>%
  gather(X, Concentration, Concentration, `Concentration (additional)`, na.rm = TRUE) %>%
  group_by(Concentration) %>%
  provide_statistics()

# Year level
statistics_year <- d_transcript %>%
  group_by(Year_numerical) %>%
  provide_statistics()

# Level level
statistics_level <- d_transcript %>%
  # TODO: filter for student who completed their studies
  inner_join(d_course, by = c("Course ID")) %>%
  filter(!is.na(Level)) %>%
  group_by(Level) %>%
  provide_statistics()

#
# output
save(statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level, file = "Output/Transcript Statistics.RDATA")

rm(provide_statistics, statistics_student, statistics_course, statistics_cluster,
     statistics_concentration, statistics_year, statistics_level)
```

# Association Rules and Sequence Rules
For a first exploration of arules, we conceptualise our framework like this:
transaction = student
item = course

## Data Prep: Creating Transactions and Sequences
First we transform our data into transaction data. For this, we first create a vector of mandatory courses that we exclude from transcripts. 

### take, fail, low grade
```{r AR data prep 1}

#
# Threshold: pass grade, high grade
pass_grade <- 5.5
high_grade <- 6.5


#
# Transactions
d_transactions <- d_course %>%
  
  # Exclude 
  filter(
    Type != "Mandatory",                  # (i) mandatory courses e.g. COR, CAP, etc
    ! Letters %in% c("SKI", "PRO",        # (ii) skills & projects (taken by majority of students)
                     "SAS", "SAH", "SAC") # (iii) courses of semester abroad (uninformative)
    ) %>%
  
  # Join with transcripts.
  select(- Period) %>%
  inner_join(
    d_transcript,
    by = "Course ID"
    ) %>%
  
  # Identifying sequenceID
  rename(sequenceID = `Student ID`) %>%
  
  # Identifying evenID
  mutate(
    Period  = substr(Period, 1, 1),
    eventID = as.numeric(paste(Year_numerical, Period, sep = ""))
    ) %>%

  # Identifying itemID
  mutate(
    PF = case_when( Grade <  pass_grade ~ "fail",
                    Grade >= pass_grade ~ "pass"),
    HL = case_when( Grade <  high_grade ~ "low",
                    Grade >= high_grade ~ "high"),
    
    item    = `Course ID`,
    item_PF = paste(`Course ID`, PF, sep = "_"),
    item_HL = paste(`Course ID`, HL, sep = "_")
    )

```

### take / not take, pass / fail / not take
```{r transactions not taken}
d_transactions_TPF <- expand.grid(
  
  # Expand along students (sequenceID) and courses (itemID)
  sequenceID = unique(d_transactions$sequenceID),
  item       = unique(d_transactions$item),
  stringsAsFactors = FALSE
  ) %>%
  
  # Join with d_transactions
  left_join(
    d_transactions, 
    by = c("sequenceID", "item")
    ) %>%
  
  # Create 
  mutate(
    
    TPF = case_when(
      is.na(Grade)       ~ "not taken",
      Grade < pass_grade ~ "fail",
      TRUE               ~ "pass"
      ),

    item_TPF = paste(item, TPF, sep = "_")
    
    )

rm(high_grade, pass_grade)
```

### Grade
```{r transactions expanded}

d_transactions_G <- d_transactions %>%
  
  # Spread along rounded grades
  mutate(
    Round_grade = round(Grade, 0),
    Values      = TRUE
    ) %>%
  spread(
    key   = Round_grade, 
    value = Values
    ) %>%
  
  # fill grades inferior to obtained grade with TRUE
  mutate(
    `1`  = `0`|`1`,
    `2`  = `1`|`2`,
    `3`  = `2`|`3`,
    `4`  = `3`|`4`,
    `5`  = `4`|`5`,
    `6`  = `5`|`6`,
    `7`  = `6`|`7`,
    `8`  = `7`|`8`,
    `9`  = `8`|`9`,
    `10` = `9`|`10`
    ) %>%
  
  # Gather along rounded grades
  gather(
    key   = Grade_round,
    value = X,
    `0`, `1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`, `10`, 
    na.rm   = TRUE, 
    convert = TRUE
    ) %>%
  select(
    -X
    )%>%

  # Identifying item
  mutate(
    item_G = paste(item, Grade_round, sep = "_")
    )
```
### Additional General Dataframes for courses
#### Transcript with Preceding courses per student
```{r}
d_cumulative_transcript <-  d_transactions        %>%
  select(sequenceID, item, eventID)  %>%
  arrange(sequenceID, eventID, item) %>%
  group_by(sequenceID, eventID)      %>%
  summarize_all(paste, collapse=" ") %>%
  arrange(sequenceID, eventID)       %>%
  mutate(
    course_all  = paste(unique(d_transactions$item), collapse = " "),
    course_all  = str_split(course_all, pattern = " "),
    
    course_now  = str_split(item, pattern = " "),

    course_past     = Reduce(paste, item, accumulate=T),
    course_past     = lag(course_past),
    course_past     = str_split(course_past, pattern = " ")) %>%
  ungroup() %>%
    mutate(
    course_past_now = purrr::map2(.x = course_past,
                                      .y = course_now,
                                      .f = union),
    course_not = purrr::map2( .x = course_all,
                              .y = course_past_now,
                              .f = setdiff)
    ) %>%
  mutate(item = str_split(item, pattern = " "))
```

#### Courses that were taken before each course
```{r}
d_past_student_course <- d_cumulative_transcript %>%
  select(sequenceID, eventID, item, course_past)%>%
  unnest(item, .drop=F)

d_past_course <- expand.grid(
  
  # Expand along students (sequenceID) and courses (itemID)
  sequenceID = unique(d_transactions$sequenceID),
  item       = unique(d_transactions$item),
  stringsAsFactors = FALSE
  ) %>%
  full_join(d_past_student_course, by= c("sequenceID", "item")) %>%  #ATTENTION: Duplicates resits. Adds values per retake of course (eg.10177849 SCI3048)
  arrange(sequenceID) %>%
  ungroup() %>%
  drop_na(eventID) %>%
  unnest(course_past) %>%
  distinct() %>%
  group_by(item) %>%
  summarise(full_past = paste( course_past, collapse = " ")) %>% #difference, between this and above?
  mutate(full_past    = str_split(full_past, pattern = " "),
         full_past    = map(.f= unique, .x=full_past),
         total_past_courses = map(.f=length, .x=full_past))
```

### Support

```{r}

# Number of students
n_students <- d_transactions %>%
  select(sequenceID) %>%
  n_distinct


# Probability of taking a course
d_support_take <- d_transactions %>%
  distinct(
    item,
    sequenceID
    ) %>%
  count(
    item
    ) %>%
  mutate(
    rate.take = n / n_students,
    rate.not  = 1 - rate.take)

# Probability of failing, having a low grade, having a grade less than x per course
d_support <- d_transactions %>%
  mutate(
    grade_round = round(Grade)
  ) %>%
  group_by(
    item
    ) %>%
  summarise(
    rate.fail = mean(PF == "fail"),
    rate.low  = mean(HL == "low"),
    rate.0    = mean(grade_round <= 0),
    rate.1    = mean(grade_round <= 1),
    rate.2    = mean(grade_round <= 2),
    rate.3    = mean(grade_round <= 3),
    rate.4    = mean(grade_round <= 4),
    rate.5    = mean(grade_round <= 5),
    rate.6    = mean(grade_round <= 6),
    rate.7    = mean(grade_round <= 7),
    rate.8    = mean(grade_round <= 8),
    rate.9    = mean(grade_round <= 9),
    rate.10   = mean(grade_round <= 10)
    )

```


### Transactions for Apriori
```{r making transactions, message = FALSE}

#
# For convenience
make_transaction <- function(data = d_transactions, item = item){
  
  data %>%
    group_by(
      sequenceID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
}


#
# Making transactions
transactions     <- make_transaction(item = item   )
transactions_PF  <- make_transaction(item = item_PF)
transactions_HL  <- make_transaction(item = item_HL)
transactions_TPF <- make_transaction(data = d_transactions_TPF, item = item_TPF)
transactions_G   <- make_transaction(data = d_transactions_G  , item = item_G  )


#
# Checking transactions
#inspect(head(transactions_Grade, 10))

rm(make_transaction)
```

### Sequences for CSPADE
```{r making sequences}

#
# For convenience
make_sequence <- function(data = d_transactions, item = item){
  
  sequences <- data %>%
    filter(!is.na(eventID))%>% #this is here casue we have courses that were never taken.
    group_by(
      sequenceID,
      eventID
    ) %>%
    summarise(
      list_item = list(!!enquo(item))
    ) %>%
    arrange(
      sequenceID,
      eventID
    ) %>%
    ungroup %>%
    pull(
      list_item
      ) %>%
    as("transactions")
  
  sequences@itemsetInfo <- select(
    data,
    sequenceID,
    eventID
    ) %>%
    as.data.frame
  
  return(sequences)
  
}


#
# Making sequences
sequences     <- make_sequence(item = item   )
sequences_PF  <- make_sequence(item = item_PF)
sequences_HL  <- make_sequence(item = item_HL)
#sequences_TNT <- make_sequence(data = d_transactions_not_taken, item = item_TNT) # compute ad hoc
#sequences_TPF <- make_sequence(data = d_transactions_not_taken, item = item_TPF) # compute ad hoc
sequences_G   <- make_sequence(data = d_transactions_G    , item = item_G  )


#
# Checking sequences
# inspect(head(sequences_G, 10))


rm(make_sequence)
```


## Mining Rules
### Apriori Algorithm

The function `my_apriori()` applies the apriori algorithm on a set of transactions with the parameters that we have chosen.
```{r my_apriori}

my_apriori <- function(data){
  
  data %>%
    apriori(
      
      # Chosen parameters
      parameter = list(
        supp = 0,    # min support
        smax = 1,    # max support
        conf = 0,    # min confidence
        minlen = 2,  # min length of 
        maxlen = 2  # max length of rule
        ),
      
      # no printing during execution
      control = list(
        verbose = FALSE
        )
      
      )
  
}
```

The function `clean_AR()` transforms the rules generated by the function `my_apriori()` into a readable dataframe
```{r clean_AR}

clean_AR <- function(AR){
  
  AR %>%
    as("data.frame") %>%
    
    # Exclude rules with no support
    filter(
      count >= 1
      ) %>%
    
    # Clean variable `rules`
    mutate(
      rules = str_remove_all(rules, pattern = "[{]"),
      rules = str_remove_all(rules, pattern = "[}]")
      ) %>%
    separate(
      rules,
      into = c("lhs", "rhs"), 
      sep  = " => "
      )
  
}
```

The function `compute_support()` computes the support of the lhs and rhs of the rules.

```{r compute_support}
compute_metrics <- function(AR, data_support, type_rule){
  
  type_rule_enquo <- enquo(type_rule)
  
  AR %>%
    
    # rhs.support
    mutate(
      rhs_course = str_remove_all(rhs, pattern = "_fail|_pass|_low|_not taken")
      ) %>%
    left_join(
      select(data_support, item, !!type_rule_enquo),
      by = c("rhs_course" = "item")
      ) %>%
    mutate(
      rhs.support = !!type_rule_enquo
      )

}
```

```{r editing_AR}
editing_AR <- function(AR){
  
  AR %>%
    mutate_at(
      c("support", 
        "confidence", "rhs.support",
        "lift"
        ),
      funs(round(., 5))
    ) %>%
    select(
      lhs, rhs,
      support, count,
      confidence, rhs.support,
      lift
    ) %>%
    arrange(
      desc(count)
    )
  
}
```


We encapsulate the three functions `my_apriori()`, `clean_AR()` and `compute_support` into the function `make_AR()`.

```{r make_AR}
make_AR <- function(data,
                    data_support, type_rule){
  
  data %>%
    my_apriori %>%
    clean_AR %>%
    compute_metrics(
      data_support = data_support,
      type_rule = !!enquo(type_rule)
      ) %>%
    editing_AR
  
}
```


```{r AR, cache = TRUE}

#
# AR taken (confidence and lift already correct)
AR_taken <- transactions %>%
  make_AR(
    data_support = d_support_take, 
    type_rule = rate.take
    )

#
# AR fail
AR_PF <- transactions_PF %>%
  make_AR(
    data_support = d_support,
    type_rule = rate.fail
    ) %>%
  
  # Confidence and Lift
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  group_by(
    lhs, rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = sum(support),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / rhs.support
    ) %>%
  ungroup %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "fail"),
    str_detect(rhs, "fail"),
    count >= 5
    ) %>%
  select(
    - c(rhs_outcome, rhs_course)
  ) 


#
# AR low
AR_HL <- transactions_HL %>%
  make_AR(
    data_support = d_support,
    type_rule = rate.low
    ) %>%
  
  # Confidence and Lift
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  group_by(
    lhs, rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = sum(support),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / rhs.support
    ) %>%
  ungroup %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "low"),
    str_detect(rhs, "low"),
    count >= 5
    ) %>%
  select(
    - c(rhs_outcome, rhs_course)
  )

  
#
# AR not taken / fail / pass

AR_TPF <- transactions_TPF %>%
  make_AR(
    data_support = d_support, 
    type_rule = rate.fail
  ) %>%
  
  # Confidence and Lift
  separate(
    rhs,
    sep = "_",
    into = c("rhs_course", "rhs_outcome"),
    remove = FALSE
    ) %>%
  mutate(
    rhs.take = str_detect(rhs, "fail|pass")
    ) %>%
  group_by(
    lhs, rhs_course
    ) %>%
  mutate(
    lhs.rhsTake.support = sum(support * rhs.take),
    confidence          = support / lhs.rhsTake.support,
    lift                = confidence / (rhs.support + 1e-4) # increase denominator by smal value to avoid dividing by 0.
    ) %>%
  ungroup %>%
  
  # lhs and rhs must be fail, count >= 5
  filter(
    str_detect(lhs, "not taken"),
    str_detect(rhs, "fail"),
    count >= 5
    ) %>%
  select(
    - c(rhs_outcome, rhs_course, rhs.take)
  )


rm(transactions, transactions_PF, transactions_HL, transactions_TPF)
```

```{r AR grade (by hand)}

#
# d_support
d_support_grade <- d_support %>%
  gather(
    key   = grade_round,
    value = support.grade,
    rate.0, rate.1, rate.2, rate.3, rate.4, rate.5, rate.6, rate.7, rate.8, rate.9, rate.10
    ) %>%
  mutate( 
    grade_round = substr(grade_round, start = 6, stop = 7),
    grade_round = as.numeric(grade_round)
    ) %>%
  select(-c(rate.fail, rate.low))


# Make AR
AR_G <- transactions_G %>%
  my_apriori(appearance = NULL) %>%
  clean_AR %>%

  # include rhs.support by hand
  separate(
    rhs, 
    into = c("item", "grade_round"), 
    sep = "_",
    remove = FALSE,
    convert = TRUE
    ) %>%
  left_join(
    d_support_grade,
    by = c("item", "grade_round")
    ) %>%
  mutate(
    rhs.support = support.grade
    ) %>%
  
    editing_AR


rm(d_support_grade, transactions_G)
```

We save and remove objects:
```{r saving and rm}
#
# Save association rules
save(AR_taken, AR_PF, AR_HL, AR_TPF, AR_G,
     file = "App/AR.RDATA")

# Remove objects
rm(my_apriori, clean_AR, compute_metrics, make_AR,
   AR_taken, AR_PF, AR_HL, AR_TPF, AR_G)
```

### CSPADE Algorithm
##TODO: fine tune parameters of cspade and ruleInduction, not taken --> fail

```{r function clean SR}
# Transform rules from ruleInduction into a readable data frame

clean_SR<- function(rules){
  
  rules %>%
    as("data.frame") %>%
    mutate(
      rule = str_remove_all(rule, pattern = "[<]"),
      rule = str_remove_all(rule, pattern = "[{]"),
      rule = str_remove_all(rule, pattern = "[}]"),
      rule = str_remove_all(rule, pattern = "[>]")
      ) %>%
    separate(rule, into = c("lhs", "rhs"), sep="=")  %>%
    select(
      lhs, rhs,
      support,
      confidence,
      lift
    ) %>%
    separate(lhs, into=c("LHS Course ID", "LHS Quality"), sep="_", remove= F) %>%
    separate(rhs, into=c("RHS Course ID", "RHS Quality"), sep="_", remove= F) %>%
    # Compute statistic
    group_by(lhs,`RHS Course ID`) %>%
    mutate(
      lhs.rhsCourse.support = sum(support),
      confidence = case_when(is.na(`RHS Quality`) ~ confidence,
                             T ~ (support / lhs.rhsCourse.support)),
      count = support * n_students,
      lhs.rhsCourse.count = lhs.rhsCourse.support * n_students,
      lift = 1 #dummy variable
      # TODO: compute rhs.support
      # lift = confidence.corr / rhs.support
     ) %>%
    ungroup() %>%
    mutate_at(
      c("support",
        "confidence", "lift"),
      funs(round(., 5))
    ) %>%
    filter(!`LHS Course ID`==`RHS Course ID`) %>%
    select(-`LHS Course ID`, -`LHS Quality`, -`RHS Course ID`, -`RHS Quality`)
    
}

```
We apply cspade followed by rule induction
```{r my_SR function}
# Run the cspade algorithm with desired parameters. Then run the ruleInduction algorithm with desired parameters.
my_SR<- function(data){
 data %>%
  cspade(
    parameter = list(
      support = 0, # min support of a sequence
      maxsize = 1, # max number of items of an element of a sequence
      maxlen  = 2, # max number of element of a sequence
      mingap  = 1, # min time difference between consecutive element of a sequence
      maxgap  = 1e4 # max time difference between consecutive element of a sequence
      #maxwin  = 1e4  # max time difference between any two elements of a sequence # WARNING: 'maxwin' disabled
      ),
    control = list(
      verbose = FALSE
      )
    ) %>%
    ruleInduction(
      confidence = 0, #we need to play with the parameters.
      control    = list(verbose = FALSE)
      ) %>%
    clean_SR
}
```

We generate sequence rules:
```{r SR taken, cache = TRUE}

#Generating sequence rules:
SR_taken   <- my_SR(data = sequences   )
```

```{r SR PF, cache = TRUE}

##pass/fail filter
SR_PF      <- my_SR(data = sequences_PF) %>%
  filter(
    str_detect(rhs, "fail"),
    str_detect(lhs, "fail")
    )
```

```{r SR HL, cache = TRUE}

##high/low filter
SR_HL      <- my_SR(data = sequences_HL) %>% 
  filter(
    str_detect(rhs, "low"),
    str_detect(lhs, "low")
    )
```

```{r SR grade, cache = TRUE}

SR_Grade   <- my_SR(data = sequences_Grade )
```

```{r SR grade exp, cache = TRUE}

#Grades filter 
SR_expanded_Grade   <- my_SR(data = sequences_expanded_Grade )
```

We save and remove objects:
```{r saving and rm SR}
#Saving sequence rules
save(
    SR_taken, SR_PF, SR_HL, SR_Grade,
    SR_expanded_Grade,
    file = "App/SR.RDATA")

#clean unnecessary objects
rm(clean_SR, my_SR, 
   sequences, sequences_PF, sequences_HL, sequences_Grade,
   sequences_expanded_Grade,
   n_students,
   SR_taken, SR_PF, SR_HL, SR_Grade,

   SR_expanded_Grade)
```


